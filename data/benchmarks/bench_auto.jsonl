{"qid": "q1_extract", "query": "What is Logistic Regression Background?", "gold_chunk_id": "5_LR_Apr_7_2021_s1_c1", "gold_span": "Logistic Regression Background: Generative and Discriminative Classifiers In this lecture we talk about the difference between generative and discriminative classifiers, and the relationship between Naïve Bayes and Logistic Regression.", "category": "extractive", "doc_version": "v1"}
{"qid": "q2_paraphrase", "query": "What is Logistic Regression Background?", "gold_chunk_id": "5_LR_Apr_7_2021_s1_c1", "gold_span": "Logistic Regression Background: Generative and Discriminative Classifiers In this lecture we talk about the difference between generative and discriminative classifiers, and the relationship between Naïve Bayes and Logistic Regression.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q3_paraphrase", "query": "Please provide details about Logistic Regression Background.", "gold_chunk_id": "5_LR_Apr_7_2021_s1_c1", "gold_span": "Logistic Regression Background: Generative and Discriminative Classifiers In this lecture we talk about the difference between generative and discriminative classifiers, and the relationship between Naïve Bayes and Logistic Regression.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q4_extract", "query": "What is Generative Classifier?", "gold_chunk_id": "5_LR_Apr_7_2021_s5_c5", "gold_span": "Generative Classifier: Generative Classifier: Build a model of what's in a cat image Knows about whiskers, ears, eyes Assigns a probability to any image: how cat-y is this image? Also build a model for dog images Now given a new image: Run both models and see which one fits better", "category": "extractive", "doc_version": "v1"}
{"qid": "q5_paraphrase", "query": "Can you tell me about the Generative Classifier?", "gold_chunk_id": "5_LR_Apr_7_2021_s5_c5", "gold_span": "Generative Classifier: Generative Classifier: Build a model of what's in a cat image Knows about whiskers, ears, eyes Assigns a probability to any image: how cat-y is this image? Also build a model for dog images Now given a new image: Run both models and see which one fits better", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q6_paraphrase", "query": "Please provide details about Generative Classifier.", "gold_chunk_id": "5_LR_Apr_7_2021_s5_c5", "gold_span": "Generative Classifier: Generative Classifier: Build a model of what's in a cat image Knows about whiskers, ears, eyes Assigns a probability to any image: how cat-y is this image? Also build a model for dog images Now given a new image: Run both models and see which one fits better", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q7_extract", "query": "What is Logistic Regression Background?", "gold_chunk_id": "5_LR_Apr_7_2021_s10_c10", "gold_span": "Logistic Regression Background: Generative and Discriminative Classifiers We've now seen the high-level intuition of the components of logistic regression and its relationship to the other classifier we've learned about, Naïve Bayes", "category": "extractive", "doc_version": "v1"}
{"qid": "q8_paraphrase", "query": "Give me the Logistic Regression Background.", "gold_chunk_id": "5_LR_Apr_7_2021_s10_c10", "gold_span": "Logistic Regression Background: Generative and Discriminative Classifiers We've now seen the high-level intuition of the components of logistic regression and its relationship to the other classifier we've learned about, Naïve Bayes", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q9_paraphrase", "query": "Please provide details about Logistic Regression Background.", "gold_chunk_id": "5_LR_Apr_7_2021_s10_c10", "gold_span": "Logistic Regression Background: Generative and Discriminative Classifiers We've now seen the high-level intuition of the components of logistic regression and its relationship to the other classifier we've learned about, Naïve Bayes", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q10_extract", "query": "What is Text Classification?", "gold_chunk_id": "5_LR_Apr_7_2021_s13_c13", "gold_span": "Text Classification: definition", "category": "extractive", "doc_version": "v1"}
{"qid": "q11_paraphrase", "query": "What is Text Classification?", "gold_chunk_id": "5_LR_Apr_7_2021_s13_c13", "gold_span": "Text Classification: definition", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q12_paraphrase", "query": "Give me the Text Classification.", "gold_chunk_id": "5_LR_Apr_7_2021_s13_c13", "gold_span": "Text Classification: definition", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q13_extract", "query": "What is The problem?", "gold_chunk_id": "5_LR_Apr_7_2021_s19_c19", "gold_span": "The problem: z isn't a probability, it's just a number! The problem: z isn't a probability, it's just a number! Solution: use a function of z that goes from 0 to 1 nothing in Eq. 5.3 forces z to be a legal probability, that is, to lie between 0 and 1. In fact, since weights are real-valued, the output might even be negative; z ranges from −∞ to ∞.", "category": "extractive", "doc_version": "v1"}
{"qid": "q14_paraphrase", "query": "What is The problem?", "gold_chunk_id": "5_LR_Apr_7_2021_s19_c19", "gold_span": "The problem: z isn't a probability, it's just a number! The problem: z isn't a probability, it's just a number! Solution: use a function of z that goes from 0 to 1 nothing in Eq. 5.3 forces z to be a legal probability, that is, to lie between 0 and 1. In fact, since weights are real-valued, the output might even be negative; z ranges from −∞ to ∞.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q15_paraphrase", "query": "Give me the The problem.", "gold_chunk_id": "5_LR_Apr_7_2021_s19_c19", "gold_span": "The problem: z isn't a probability, it's just a number! The problem: z isn't a probability, it's just a number! Solution: use a function of z that goes from 0 to 1 nothing in Eq. 5.3 forces z to be a legal probability, that is, to lie between 0 and 1. In fact, since weights are real-valued, the output might even be negative; z ranges from −∞ to ∞.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q16_extract", "query": "What is By the way?", "gold_chunk_id": "5_LR_Apr_7_2021_s23_c23", "gold_span": "By the way: By the way: = Because The sigmoid function has the property", "category": "extractive", "doc_version": "v1"}
{"qid": "q17_paraphrase", "query": "Give me the By the way.", "gold_chunk_id": "5_LR_Apr_7_2021_s23_c23", "gold_span": "By the way: By the way: = Because The sigmoid function has the property", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q18_paraphrase", "query": "Summarize the By the way.", "gold_chunk_id": "5_LR_Apr_7_2021_s23_c23", "gold_span": "By the way: By the way: = Because The sigmoid function has the property", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q19_extract", "query": "What is Logistic Regression Logistic Regression?", "gold_chunk_id": "5_LR_Apr_7_2021_s28_c28", "gold_span": "Logistic Regression Logistic Regression: a text example on sentiment classification Let's walk through an example using logistic regression to do sentiment classification", "category": "extractive", "doc_version": "v1"}
{"qid": "q20_paraphrase", "query": "Give me the Logistic Regression Logistic Regression.", "gold_chunk_id": "5_LR_Apr_7_2021_s28_c28", "gold_span": "Logistic Regression Logistic Regression: a text example on sentiment classification Let's walk through an example using logistic regression to do sentiment classification", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q21_paraphrase", "query": "Please provide details about Logistic Regression Logistic Regression.", "gold_chunk_id": "5_LR_Apr_7_2021_s28_c28", "gold_span": "Logistic Regression Logistic Regression: a text example on sentiment classification Let's walk through an example using logistic regression to do sentiment classification", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q22_extract", "query": "What is Sentiment example?", "gold_chunk_id": "5_LR_Apr_7_2021_s29_c29", "gold_span": "Sentiment example: does y=1 or y=0? Sentiment example: does y=1 or y=0? It's hokey . There are virtually no surprises , and the writing is second-rate . So why was it so enjoyable ? For one thing , the cast is great . Another nice touch is the music . I was overcome with the urge to get off the couch and start dancing . It sucked me in , and it'll do the same to you . 29 Suppose we are doing binary sentiment classification on movie review text, and we would like to know whether to assign the sentiment class 1=positive or 0=negative to the following review", "category": "extractive", "doc_version": "v1"}
{"qid": "q23_paraphrase", "query": "Can you tell me about the Sentiment example?", "gold_chunk_id": "5_LR_Apr_7_2021_s29_c29", "gold_span": "Sentiment example: does y=1 or y=0? Sentiment example: does y=1 or y=0? It's hokey . There are virtually no surprises , and the writing is second-rate . So why was it so enjoyable ? For one thing , the cast is great . Another nice touch is the music . I was overcome with the urge to get off the couch and start dancing . It sucked me in , and it'll do the same to you . 29 Suppose we are doing binary sentiment classification on movie review text, and we would like to know whether to assign the sentiment class 1=positive or 0=negative to the following review", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q24_paraphrase", "query": "Give me the Sentiment example.", "gold_chunk_id": "5_LR_Apr_7_2021_s29_c29", "gold_span": "Sentiment example: does y=1 or y=0? Sentiment example: does y=1 or y=0? It's hokey . There are virtually no surprises , and the writing is second-rate . So why was it so enjoyable ? For one thing , the cast is great . Another nice touch is the music . I was overcome with the urge to get off the couch and start dancing . It sucked me in , and it'll do the same to you . 29 Suppose we are doing binary sentiment classification on movie review text, and we would like to know whether to assign the sentiment class 1=positive or 0=negative to the following review", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q25_extract", "query": "What is We can build features for logistic regression for any classification task?", "gold_chunk_id": "5_LR_Apr_7_2021_s33_c33", "gold_span": "We can build features for logistic regression for any classification task: period disambiguation We can build features for logistic regression for any classification task: period disambiguation 33 This ends in a period. The house at 465 Main St. is new. End of sentence Not end We might use features like x1 below expressing that the current word is lower case and the class is EOS (perhaps with a positive weight), or that the current word is in our abbreviations dictionary (“Prof.”) and the class is EOS (perhaps with a negative weight). A feature can also express a quite complex combination of properties. For example a period following an upper case word is likely to be an EOS, but if the word itself is St. and the previous word is capitalized, then the period is likely part of a shortening of the word street.", "category": "extractive", "doc_version": "v1"}
{"qid": "q26_paraphrase", "query": "Give me the We can build features for logistic regression for any classification task.", "gold_chunk_id": "5_LR_Apr_7_2021_s33_c33", "gold_span": "We can build features for logistic regression for any classification task: period disambiguation We can build features for logistic regression for any classification task: period disambiguation 33 This ends in a period. The house at 465 Main St. is new. End of sentence Not end We might use features like x1 below expressing that the current word is lower case and the class is EOS (perhaps with a positive weight), or that the current word is in our abbreviations dictionary (“Prof.”) and the class is EOS (perhaps with a negative weight). A feature can also express a quite complex combination of properties. For example a period following an upper case word is likely to be an EOS, but if the word itself is St. and the previous word is capitalized, then the period is likely part of a shortening of the word street.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q27_paraphrase", "query": "Summarize the We can build features for logistic regression for any classification task.", "gold_chunk_id": "5_LR_Apr_7_2021_s33_c33", "gold_span": "We can build features for logistic regression for any classification task: period disambiguation We can build features for logistic regression for any classification task: period disambiguation 33 This ends in a period. The house at 465 Main St. is new. End of sentence Not end We might use features like x1 below expressing that the current word is lower case and the class is EOS (perhaps with a positive weight), or that the current word is in our abbreviations dictionary (“Prof.”) and the class is EOS (perhaps with a negative weight). A feature can also express a quite complex combination of properties. For example a period following an upper case word is likely to be an EOS, but if the word itself is St. and the previous word is capitalized, then the period is likely part of a shortening of the word street.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q28_extract", "query": "What is Classification in (binary) logistic regression?", "gold_chunk_id": "5_LR_Apr_7_2021_s34_c34", "gold_span": "Classification in (binary) logistic regression: summary Classification in (binary) logistic regression: summary Given: a set of classes: (+ sentiment,- sentiment) a vector x of features [x1, x2, …, xn] x1= count( \"awesome\") x2 = log(number of words in review) A vector w of weights [w1, w2, …, wn] wi for each feature fi", "category": "extractive", "doc_version": "v1"}
{"qid": "q29_paraphrase", "query": "Give me the Classification in (binary) logistic regression.", "gold_chunk_id": "5_LR_Apr_7_2021_s34_c34", "gold_span": "Classification in (binary) logistic regression: summary Classification in (binary) logistic regression: summary Given: a set of classes: (+ sentiment,- sentiment) a vector x of features [x1, x2, …, xn] x1= count( \"awesome\") x2 = log(number of words in review) A vector w of weights [w1, w2, …, wn] wi for each feature fi", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q30_paraphrase", "query": "Can you tell me about the Classification in (binary) logistic regression?", "gold_chunk_id": "5_LR_Apr_7_2021_s34_c34", "gold_span": "Classification in (binary) logistic regression: summary Classification in (binary) logistic regression: summary Given: a set of classes: (+ sentiment,- sentiment) a vector x of features [x1, x2, …, xn] x1= count( \"awesome\") x2 = log(number of words in review) A vector w of weights [w1, w2, …, wn] wi for each feature fi", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q31_extract", "query": "What is Logistic Regression Logistic Regression?", "gold_chunk_id": "5_LR_Apr_7_2021_s35_c35", "gold_span": "Logistic Regression Logistic Regression: a text example on sentiment classification We've now seen the details of how logistic regression can take feature values and weights to assign a class to an input.", "category": "extractive", "doc_version": "v1"}
{"qid": "q32_paraphrase", "query": "What is Logistic Regression Logistic Regression?", "gold_chunk_id": "5_LR_Apr_7_2021_s35_c35", "gold_span": "Logistic Regression Logistic Regression: a text example on sentiment classification We've now seen the details of how logistic regression can take feature values and weights to assign a class to an input.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q33_paraphrase", "query": "Give me the Logistic Regression Logistic Regression.", "gold_chunk_id": "5_LR_Apr_7_2021_s35_c35", "gold_span": "Logistic Regression Logistic Regression: a text example on sentiment classification We've now seen the details of how logistic regression can take feature values and weights to assign a class to an input.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q34_extract", "query": "What is Logistic Regression Learning?", "gold_chunk_id": "5_LR_Apr_7_2021_s36_c36", "gold_span": "Logistic Regression Learning: Cross-Entropy Loss Let's now turn to learning the parameters for logistic regression. We'll start with the cross-entropy loss function", "category": "extractive", "doc_version": "v1"}
{"qid": "q35_paraphrase", "query": "What is Logistic Regression Learning?", "gold_chunk_id": "5_LR_Apr_7_2021_s36_c36", "gold_span": "Logistic Regression Learning: Cross-Entropy Loss Let's now turn to learning the parameters for logistic regression. We'll start with the cross-entropy loss function", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q36_paraphrase", "query": "Please provide details about Logistic Regression Learning.", "gold_chunk_id": "5_LR_Apr_7_2021_s36_c36", "gold_span": "Logistic Regression Learning: Cross-Entropy Loss Let's now turn to learning the parameters for logistic regression. We'll start with the cross-entropy loss function", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q37_extract", "query": "What is Learning components Learning components A loss function?", "gold_chunk_id": "5_LR_Apr_7_2021_s38_c38", "gold_span": "Learning components Learning components A loss function: cross-entropy loss An optimization algorithm: stochastic gradient descent This requires two components. The first is a metric for how close the current label (yˆ) is to the true gold label y. Rather than measure similarity, we usually talk about the opposite of this: the distance between the system output and the gold output, and we call this distance the loss function or the cost function. We'll we’ll introduce the loss function that is commonly used for logistic regression and also for neural networks, the cross-entropy loss. The second thing we need is an optimization algorithm for iteratively updating the weights so as to minimize this loss function. The standard algorithm for this is gradient descent; we’ll introduce the stochastic gradient descent algorithm in the following section.", "category": "extractive", "doc_version": "v1"}
{"qid": "q38_paraphrase", "query": "Give me the Learning components Learning components A loss function.", "gold_chunk_id": "5_LR_Apr_7_2021_s38_c38", "gold_span": "Learning components Learning components A loss function: cross-entropy loss An optimization algorithm: stochastic gradient descent This requires two components. The first is a metric for how close the current label (yˆ) is to the true gold label y. Rather than measure similarity, we usually talk about the opposite of this: the distance between the system output and the gold output, and we call this distance the loss function or the cost function. We'll we’ll introduce the loss function that is commonly used for logistic regression and also for neural networks, the cross-entropy loss. The second thing we need is an optimization algorithm for iteratively updating the weights so as to minimize this loss function. The standard algorithm for this is gradient descent; we’ll introduce the stochastic gradient descent algorithm in the following section.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q39_paraphrase", "query": "What is Learning components Learning components A loss function?", "gold_chunk_id": "5_LR_Apr_7_2021_s38_c38", "gold_span": "Learning components Learning components A loss function: cross-entropy loss An optimization algorithm: stochastic gradient descent This requires two components. The first is a metric for how close the current label (yˆ) is to the true gold label y. Rather than measure similarity, we usually talk about the opposite of this: the distance between the system output and the gold output, and we call this distance the loss function or the cost function. We'll we’ll introduce the loss function that is commonly used for logistic regression and also for neural networks, the cross-entropy loss. The second thing we need is an optimization algorithm for iteratively updating the weights so as to minimize this loss function. The standard algorithm for this is gradient descent; we’ll introduce the stochastic gradient descent algorithm in the following section.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q40_extract", "query": "What is Our goal?", "gold_chunk_id": "5_LR_Apr_7_2021_s50_c50", "gold_span": "Our goal: minimize the loss Our goal: minimize the loss", "category": "extractive", "doc_version": "v1"}
{"qid": "q41_paraphrase", "query": "Give me the Our goal.", "gold_chunk_id": "5_LR_Apr_7_2021_s50_c50", "gold_span": "Our goal: minimize the loss Our goal: minimize the loss", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q42_paraphrase", "query": "What is Our goal?", "gold_chunk_id": "5_LR_Apr_7_2021_s50_c50", "gold_span": "Our goal: minimize the loss Our goal: minimize the loss", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q43_extract", "query": "What is Our goal?", "gold_chunk_id": "5_LR_Apr_7_2021_s52_c52", "gold_span": "Our goal: minimize the loss Our goal: minimize the loss For logistic regression, loss function is convex A convex function has just one minimum Gradient descent starting from any point is guaranteed to find the minimum (Loss for neural networks is non-convex) For logistic regression, this loss function is conveniently convex. A convex function has just one minimum; there are no local minima to get stuck in, so gradient descent starting from any point is guaranteed to find the minimum. (By contrast, the loss for multi-layer neural networks is non-convex, and gradient descent may get stuck in local minima for neural network training and never find the global optimum.)", "category": "extractive", "doc_version": "v1"}
{"qid": "q44_paraphrase", "query": "Give me the Our goal.", "gold_chunk_id": "5_LR_Apr_7_2021_s52_c52", "gold_span": "Our goal: minimize the loss Our goal: minimize the loss For logistic regression, loss function is convex A convex function has just one minimum Gradient descent starting from any point is guaranteed to find the minimum (Loss for neural networks is non-convex) For logistic regression, this loss function is conveniently convex. A convex function has just one minimum; there are no local minima to get stuck in, so gradient descent starting from any point is guaranteed to find the minimum. (By contrast, the loss for multi-layer neural networks is non-convex, and gradient descent may get stuck in local minima for neural network training and never find the global optimum.)", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q45_paraphrase", "query": "Summarize the Our goal.", "gold_chunk_id": "5_LR_Apr_7_2021_s52_c52", "gold_span": "Our goal: minimize the loss Our goal: minimize the loss For logistic regression, loss function is convex A convex function has just one minimum Gradient descent starting from any point is guaranteed to find the minimum (Loss for neural networks is non-convex) For logistic regression, this loss function is conveniently convex. A convex function has just one minimum; there are no local minima to get stuck in, so gradient descent starting from any point is guaranteed to find the minimum. (By contrast, the loss for multi-layer neural networks is non-convex, and gradient descent may get stuck in local minima for neural network training and never find the global optimum.)", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q46_extract", "query": "What is Logistic Regression Stochastic Gradient Descent?", "gold_chunk_id": "5_LR_Apr_7_2021_s66_c66", "gold_span": "Logistic Regression Stochastic Gradient Descent: An example and more details In this lecture we'll walk through an example of stochastic descent and give a few more details.", "category": "extractive", "doc_version": "v1"}
{"qid": "q47_paraphrase", "query": "Summarize the Logistic Regression Stochastic Gradient Descent.", "gold_chunk_id": "5_LR_Apr_7_2021_s66_c66", "gold_span": "Logistic Regression Stochastic Gradient Descent: An example and more details In this lecture we'll walk through an example of stochastic descent and give a few more details.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q48_paraphrase", "query": "Give me the Logistic Regression Stochastic Gradient Descent.", "gold_chunk_id": "5_LR_Apr_7_2021_s66_c66", "gold_span": "Logistic Regression Stochastic Gradient Descent: An example and more details In this lecture we'll walk through an example of stochastic descent and give a few more details.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q49_extract", "query": "What is Logistic Regression Stochastic Gradient Descent?", "gold_chunk_id": "5_LR_Apr_7_2021_s78_c78", "gold_span": "Logistic Regression Stochastic Gradient Descent: An example and more details We've now seen the stochastic gradient descent algorithm and discussed variants like mini-batch training.", "category": "extractive", "doc_version": "v1"}
{"qid": "q50_paraphrase", "query": "What is Logistic Regression Stochastic Gradient Descent?", "gold_chunk_id": "5_LR_Apr_7_2021_s78_c78", "gold_span": "Logistic Regression Stochastic Gradient Descent: An example and more details We've now seen the stochastic gradient descent algorithm and discussed variants like mini-batch training.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q51_paraphrase", "query": "Can you tell me about the Logistic Regression Stochastic Gradient Descent?", "gold_chunk_id": "5_LR_Apr_7_2021_s78_c78", "gold_span": "Logistic Regression Stochastic Gradient Descent: An example and more details We've now seen the stochastic gradient descent algorithm and discussed variants like mini-batch training.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q52_extract", "query": "What is Relations between senses?", "gold_chunk_id": "6_Vector_Apr18_2021_s5_c99", "gold_span": "Relations between senses: Synonymy Synonyms have the same meaning in some or all contexts. filbert / hazelnut couch / sofa big / large automobile / car vomit / throw up water / H20", "category": "extractive", "doc_version": "v1"}
{"qid": "q53_paraphrase", "query": "Please provide details about Relations between senses.", "gold_chunk_id": "6_Vector_Apr18_2021_s5_c99", "gold_span": "Relations between senses: Synonymy Synonyms have the same meaning in some or all contexts. filbert / hazelnut couch / sofa big / large automobile / car vomit / throw up water / H20", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q54_paraphrase", "query": "Summarize the Relations between senses.", "gold_chunk_id": "6_Vector_Apr18_2021_s5_c99", "gold_span": "Relations between senses: Synonymy Synonyms have the same meaning in some or all contexts. filbert / hazelnut couch / sofa big / large automobile / car vomit / throw up water / H20", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q55_extract", "query": "What is Relations between senses?", "gold_chunk_id": "6_Vector_Apr18_2021_s6_c100", "gold_span": "Relations between senses: Synonymy Note that there are probably no examples of perfect synonymy. Even if many aspects of meaning are identical Still may differ based on politeness, slang, register, genre, etc.", "category": "extractive", "doc_version": "v1"}
{"qid": "q56_paraphrase", "query": "Can you tell me about the Relations between senses?", "gold_chunk_id": "6_Vector_Apr18_2021_s6_c100", "gold_span": "Relations between senses: Synonymy Note that there are probably no examples of perfect synonymy. Even if many aspects of meaning are identical Still may differ based on politeness, slang, register, genre, etc.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q57_paraphrase", "query": "Please provide details about Relations between senses.", "gold_chunk_id": "6_Vector_Apr18_2021_s6_c100", "gold_span": "Relations between senses: Synonymy Note that there are probably no examples of perfect synonymy. Even if many aspects of meaning are identical Still may differ based on politeness, slang, register, genre, etc.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q58_extract", "query": "What is Relation?", "gold_chunk_id": "6_Vector_Apr18_2021_s7_c101", "gold_span": "Relation: Synonymy? water/H20 \"H20\" in a surfing guide? big/large my big sister != my large sister For example, the word H2O is used in scientific contexts and would be inappropriate in a surfing guide—water would be more appropriate— and this genre difference is part of the meaning of the word. In practice, the word synonym is therefore used to describe a relationship of approximate or rough synonymy.", "category": "extractive", "doc_version": "v1"}
{"qid": "q59_paraphrase", "query": "Summarize the Relation.", "gold_chunk_id": "6_Vector_Apr18_2021_s7_c101", "gold_span": "Relation: Synonymy? water/H20 \"H20\" in a surfing guide? big/large my big sister != my large sister For example, the word H2O is used in scientific contexts and would be inappropriate in a surfing guide—water would be more appropriate— and this genre difference is part of the meaning of the word. In practice, the word synonym is therefore used to describe a relationship of approximate or rough synonymy.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q60_paraphrase", "query": "Give me the Relation.", "gold_chunk_id": "6_Vector_Apr18_2021_s7_c101", "gold_span": "Relation: Synonymy? water/H20 \"H20\" in a surfing guide? big/large my big sister != my large sister For example, the word H2O is used in scientific contexts and would be inappropriate in a surfing guide—water would be more appropriate— and this genre difference is part of the meaning of the word. In practice, the word synonym is therefore used to describe a relationship of approximate or rough synonymy.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q61_extract", "query": "What is Relation?", "gold_chunk_id": "6_Vector_Apr18_2021_s10_c104", "gold_span": "Relation: Similarity Relation: Similarity Words with similar meanings. Not synonyms, but sharing some element of meaning car, bicycle cow, horse While words don’t have many synonyms, most words do have lots of similar words. Cat is not a synonym of dog, but cats and dogs are certainly similar words. The notion of word similarity is very useful in larger semantic tasks. Know- ing how similar two words are can help in computing how similar the meaning of two phrases or sentences are, a very important component of natural language un- derstanding tasks like question answering, paraphrasing, and summarization.", "category": "extractive", "doc_version": "v1"}
{"qid": "q62_paraphrase", "query": "Give me the Relation.", "gold_chunk_id": "6_Vector_Apr18_2021_s10_c104", "gold_span": "Relation: Similarity Relation: Similarity Words with similar meanings. Not synonyms, but sharing some element of meaning car, bicycle cow, horse While words don’t have many synonyms, most words do have lots of similar words. Cat is not a synonym of dog, but cats and dogs are certainly similar words. The notion of word similarity is very useful in larger semantic tasks. Know- ing how similar two words are can help in computing how similar the meaning of two phrases or sentences are, a very important component of natural language un- derstanding tasks like question answering, paraphrasing, and summarization.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q63_paraphrase", "query": "What is Relation?", "gold_chunk_id": "6_Vector_Apr18_2021_s10_c104", "gold_span": "Relation: Similarity Relation: Similarity Words with similar meanings. Not synonyms, but sharing some element of meaning car, bicycle cow, horse While words don’t have many synonyms, most words do have lots of similar words. Cat is not a synonym of dog, but cats and dogs are certainly similar words. The notion of word similarity is very useful in larger semantic tasks. Know- ing how similar two words are can help in computing how similar the meaning of two phrases or sentences are, a very important component of natural language un- derstanding tasks like question answering, paraphrasing, and summarization.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q64_extract", "query": "What is Relation?", "gold_chunk_id": "6_Vector_Apr18_2021_s12_c106", "gold_span": "Relation: Word relatedness Also called \"word association\" Words can be related in any way, perhaps via a semantic frame or field coffee, tea: similar coffee, cup: related, not similar The meaning of two words can be related in ways other than similarity. One such class of connections is called word relatedness (Budanitsky and Hirst, 2006), also traditionally called word association in psychology. Coffee is similar to tea. But coffee is not similar to cup; they share practically no features (coffee is a plant or a beverage, while a cup is a manufactured object with a particular shape). But coffee and cup are clearly related; they are associated by co-participating in an everyday event (the event of drinking coffee out of a cup). Similarly scalpel and surgeon are not similar but are related eventively (a surgeon tends to make use of a scalpel).", "category": "extractive", "doc_version": "v1"}
{"qid": "q65_paraphrase", "query": "Can you tell me about the Relation?", "gold_chunk_id": "6_Vector_Apr18_2021_s12_c106", "gold_span": "Relation: Word relatedness Also called \"word association\" Words can be related in any way, perhaps via a semantic frame or field coffee, tea: similar coffee, cup: related, not similar The meaning of two words can be related in ways other than similarity. One such class of connections is called word relatedness (Budanitsky and Hirst, 2006), also traditionally called word association in psychology. Coffee is similar to tea. But coffee is not similar to cup; they share practically no features (coffee is a plant or a beverage, while a cup is a manufactured object with a particular shape). But coffee and cup are clearly related; they are associated by co-participating in an everyday event (the event of drinking coffee out of a cup). Similarly scalpel and surgeon are not similar but are related eventively (a surgeon tends to make use of a scalpel).", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q66_paraphrase", "query": "Summarize the Relation.", "gold_chunk_id": "6_Vector_Apr18_2021_s12_c106", "gold_span": "Relation: Word relatedness Also called \"word association\" Words can be related in any way, perhaps via a semantic frame or field coffee, tea: similar coffee, cup: related, not similar The meaning of two words can be related in ways other than similarity. One such class of connections is called word relatedness (Budanitsky and Hirst, 2006), also traditionally called word association in psychology. Coffee is similar to tea. But coffee is not similar to cup; they share practically no features (coffee is a plant or a beverage, while a cup is a manufactured object with a particular shape). But coffee and cup are clearly related; they are associated by co-participating in an everyday event (the event of drinking coffee out of a cup). Similarly scalpel and surgeon are not similar but are related eventively (a surgeon tends to make use of a scalpel).", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q67_extract", "query": "What is Relation?", "gold_chunk_id": "6_Vector_Apr18_2021_s14_c108", "gold_span": "Relation: Antonymy Senses that are opposites with respect to only one feature of meaning Otherwise, they are very similar! dark/light short/long fast/slow rise/fall hot/cold up/down in/out More formally: antonyms can define a binary opposition or be at opposite ends of a scale long/short, fast/slow Be reversives: rise/fall, up/down", "category": "extractive", "doc_version": "v1"}
{"qid": "q68_paraphrase", "query": "Can you tell me about the Relation?", "gold_chunk_id": "6_Vector_Apr18_2021_s14_c108", "gold_span": "Relation: Antonymy Senses that are opposites with respect to only one feature of meaning Otherwise, they are very similar! dark/light short/long fast/slow rise/fall hot/cold up/down in/out More formally: antonyms can define a binary opposition or be at opposite ends of a scale long/short, fast/slow Be reversives: rise/fall, up/down", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q69_paraphrase", "query": "Please provide details about Relation.", "gold_chunk_id": "6_Vector_Apr18_2021_s14_c108", "gold_span": "Relation: Antonymy Senses that are opposites with respect to only one feature of meaning Otherwise, they are very similar! dark/light short/long fast/slow rise/fall hot/cold up/down in/out More formally: antonyms can define a binary opposition or be at opposite ends of a scale long/short, fast/slow Be reversives: rise/fall, up/down", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q70_extract", "query": "What is Connotation Connotation Words seem to vary along 3 affective dimensions?", "gold_chunk_id": "6_Vector_Apr18_2021_s16_c110", "gold_span": "Connotation Connotation Words seem to vary along 3 affective dimensions: valence: the pleasantness of the stimulus arousal: the intensity of emotion provoked by the stimulus dominance: the degree of control exerted by the stimulus Osgood et al. (1957) | Word | Score | | Word | Score Valence | love | 1.000 | | toxic | 0.008 | happy | 1.000 | | nightmare | 0.005 Arousal | elated | 0.960 | | mellow | 0.069 | frenzy | 0.965 | | napping | 0.046 Dominance | powerful | 0.991 | | weak | 0.045 | leadership | 0.983 | | empty | 0.081 Values from NRC VAD Lexicon (Mohammad 2018) Early work on affective meaning (Osgood et al., 1957) found that words varied along three important dimensions of affective meaning:", "category": "extractive", "doc_version": "v1"}
{"qid": "q71_paraphrase", "query": "Summarize the Connotation Connotation Words seem to vary along 3 affective dimensions.", "gold_chunk_id": "6_Vector_Apr18_2021_s16_c110", "gold_span": "Connotation Connotation Words seem to vary along 3 affective dimensions: valence: the pleasantness of the stimulus arousal: the intensity of emotion provoked by the stimulus dominance: the degree of control exerted by the stimulus Osgood et al. (1957) | Word | Score | | Word | Score Valence | love | 1.000 | | toxic | 0.008 | happy | 1.000 | | nightmare | 0.005 Arousal | elated | 0.960 | | mellow | 0.069 | frenzy | 0.965 | | napping | 0.046 Dominance | powerful | 0.991 | | weak | 0.045 | leadership | 0.983 | | empty | 0.081 Values from NRC VAD Lexicon (Mohammad 2018) Early work on affective meaning (Osgood et al., 1957) found that words varied along three important dimensions of affective meaning:", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q72_paraphrase", "query": "What is Connotation Connotation Words seem to vary along 3 affective dimensions?", "gold_chunk_id": "6_Vector_Apr18_2021_s16_c110", "gold_span": "Connotation Connotation Words seem to vary along 3 affective dimensions: valence: the pleasantness of the stimulus arousal: the intensity of emotion provoked by the stimulus dominance: the degree of control exerted by the stimulus Osgood et al. (1957) | Word | Score | | Word | Score Valence | love | 1.000 | | toxic | 0.008 | happy | 1.000 | | nightmare | 0.005 Arousal | elated | 0.960 | | mellow | 0.069 | frenzy | 0.965 | | napping | 0.046 Dominance | powerful | 0.991 | | weak | 0.045 | leadership | 0.983 | | empty | 0.081 Values from NRC VAD Lexicon (Mohammad 2018) Early work on affective meaning (Osgood et al., 1957) found that words varied along three important dimensions of affective meaning:", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q73_extract", "query": "What is Ongchoi?", "gold_chunk_id": "6_Vector_Apr18_2021_s24_c118", "gold_span": "Ongchoi: Ipomoea aquatica \"Water Spinach\" Ongchoi: Ipomoea aquatica \"Water Spinach\" Yamaguchi, Wikimedia Commons, public domain 空心菜 kangkong rau muống … Here's a picture of ongchoi for you food fans", "category": "extractive", "doc_version": "v1"}
{"qid": "q74_paraphrase", "query": "Give me the Ongchoi.", "gold_chunk_id": "6_Vector_Apr18_2021_s24_c118", "gold_span": "Ongchoi: Ipomoea aquatica \"Water Spinach\" Ongchoi: Ipomoea aquatica \"Water Spinach\" Yamaguchi, Wikimedia Commons, public domain 空心菜 kangkong rau muống … Here's a picture of ongchoi for you food fans", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q75_paraphrase", "query": "Can you tell me about the Ongchoi?", "gold_chunk_id": "6_Vector_Apr18_2021_s24_c118", "gold_span": "Ongchoi: Ipomoea aquatica \"Water Spinach\" Ongchoi: Ipomoea aquatica \"Water Spinach\" Yamaguchi, Wikimedia Commons, public domain 空心菜 kangkong rau muống … Here's a picture of ongchoi for you food fans", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q76_extract", "query": "What is Idea 1?", "gold_chunk_id": "6_Vector_Apr18_2021_s25_c119", "gold_span": "Idea 1: Defining meaning by linguistic distribution Idea 1: Defining meaning by linguistic distribution Let's define the meaning of a word by its distribution in language use, meaning its neighboring words or grammatical environments. So we can express this first intuition as Idea 1:", "category": "extractive", "doc_version": "v1"}
{"qid": "q77_paraphrase", "query": "Summarize the Idea 1.", "gold_chunk_id": "6_Vector_Apr18_2021_s25_c119", "gold_span": "Idea 1: Defining meaning by linguistic distribution Idea 1: Defining meaning by linguistic distribution Let's define the meaning of a word by its distribution in language use, meaning its neighboring words or grammatical environments. So we can express this first intuition as Idea 1:", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q78_paraphrase", "query": "What is Idea 1?", "gold_chunk_id": "6_Vector_Apr18_2021_s25_c119", "gold_span": "Idea 1: Defining meaning by linguistic distribution Idea 1: Defining meaning by linguistic distribution Let's define the meaning of a word by its distribution in language use, meaning its neighboring words or grammatical environments. So we can express this first intuition as Idea 1:", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q79_extract", "query": "What is Idea 2?", "gold_chunk_id": "6_Vector_Apr18_2021_s26_c120", "gold_span": "Idea 2: Meaning as a point in space (Osgood et al. 1957) Idea 2: Meaning as a point in space (Osgood et al. 1957) 3 affective dimensions for a word valence: pleasantness arousal: intensity of emotion dominance: the degree of control exerted Hence the connotation of a word is a vector in 3-space | Word | Score | | Word | Score Valence | love | 1.000 | | toxic | 0.008 | happy | 1.000 | | nightmare | 0.005 Arousal | elated | 0.960 | | mellow | 0.069 | frenzy | 0.965 | | napping | 0.046 Dominance | powerful | 0.991 | | weak | 0.045 | leadership | 0.983 | | empty | 0.081 NRC VAD Lexicon (Mohammad 2018) And the second idea is Osgood’s (1957) idea mentioned in the prior lecture, in which the connotation of a word is represented by 3 numbers, it's valence, arousal, and dominance. That means we are essentially representing a word's connotation by a point in three-dimensional space! But suppose we represent more than just a word's connotation", "category": "extractive", "doc_version": "v1"}
{"qid": "q80_paraphrase", "query": "What is Idea 2?", "gold_chunk_id": "6_Vector_Apr18_2021_s26_c120", "gold_span": "Idea 2: Meaning as a point in space (Osgood et al. 1957) Idea 2: Meaning as a point in space (Osgood et al. 1957) 3 affective dimensions for a word valence: pleasantness arousal: intensity of emotion dominance: the degree of control exerted Hence the connotation of a word is a vector in 3-space | Word | Score | | Word | Score Valence | love | 1.000 | | toxic | 0.008 | happy | 1.000 | | nightmare | 0.005 Arousal | elated | 0.960 | | mellow | 0.069 | frenzy | 0.965 | | napping | 0.046 Dominance | powerful | 0.991 | | weak | 0.045 | leadership | 0.983 | | empty | 0.081 NRC VAD Lexicon (Mohammad 2018) And the second idea is Osgood’s (1957) idea mentioned in the prior lecture, in which the connotation of a word is represented by 3 numbers, it's valence, arousal, and dominance. That means we are essentially representing a word's connotation by a point in three-dimensional space! But suppose we represent more than just a word's connotation", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q81_paraphrase", "query": "Summarize the Idea 2.", "gold_chunk_id": "6_Vector_Apr18_2021_s26_c120", "gold_span": "Idea 2: Meaning as a point in space (Osgood et al. 1957) Idea 2: Meaning as a point in space (Osgood et al. 1957) 3 affective dimensions for a word valence: pleasantness arousal: intensity of emotion dominance: the degree of control exerted Hence the connotation of a word is a vector in 3-space | Word | Score | | Word | Score Valence | love | 1.000 | | toxic | 0.008 | happy | 1.000 | | nightmare | 0.005 Arousal | elated | 0.960 | | mellow | 0.069 | frenzy | 0.965 | | napping | 0.046 Dominance | powerful | 0.991 | | weak | 0.045 | leadership | 0.983 | | empty | 0.081 NRC VAD Lexicon (Mohammad 2018) And the second idea is Osgood’s (1957) idea mentioned in the prior lecture, in which the connotation of a word is represented by 3 numbers, it's valence, arousal, and dominance. That means we are essentially representing a word's connotation by a point in three-dimensional space! But suppose we represent more than just a word's connotation", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q82_extract", "query": "What is Idea 1?", "gold_chunk_id": "6_Vector_Apr18_2021_s27_c121", "gold_span": "Idea 1: Defining meaning by linguistic distribution Idea 2: Meaning as a point in multidimensional space Idea 1: Defining meaning by linguistic distribution Idea 2: Meaning as a point in multidimensional space So we're going to combine these two ideas:", "category": "extractive", "doc_version": "v1"}
{"qid": "q83_paraphrase", "query": "Please provide details about Idea 1.", "gold_chunk_id": "6_Vector_Apr18_2021_s27_c121", "gold_span": "Idea 1: Defining meaning by linguistic distribution Idea 2: Meaning as a point in multidimensional space Idea 1: Defining meaning by linguistic distribution Idea 2: Meaning as a point in multidimensional space So we're going to combine these two ideas:", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q84_paraphrase", "query": "Summarize the Idea 1.", "gold_chunk_id": "6_Vector_Apr18_2021_s27_c121", "gold_span": "Idea 1: Defining meaning by linguistic distribution Idea 2: Meaning as a point in multidimensional space Idea 1: Defining meaning by linguistic distribution Idea 2: Meaning as a point in multidimensional space So we're going to combine these two ideas:", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q85_extract", "query": "What is Intuition?", "gold_chunk_id": "6_Vector_Apr18_2021_s30_c124", "gold_span": "Intuition: why vectors? Intuition: why vectors? Consider sentiment analysis: With words, a feature is a word identity Feature 5: 'The previous word was \"terrible\"' requires exact same word to be in training and test With embeddings: Feature is a word vector 'The previous word was vector [35,22,17…] Now in the test set we might see a similar vector [34,21,14] We can generalize to similar but unseen words!!!", "category": "extractive", "doc_version": "v1"}
{"qid": "q86_paraphrase", "query": "Please provide details about Intuition.", "gold_chunk_id": "6_Vector_Apr18_2021_s30_c124", "gold_span": "Intuition: why vectors? Intuition: why vectors? Consider sentiment analysis: With words, a feature is a word identity Feature 5: 'The previous word was \"terrible\"' requires exact same word to be in training and test With embeddings: Feature is a word vector 'The previous word was vector [35,22,17…] Now in the test set we might see a similar vector [34,21,14] We can generalize to similar but unseen words!!!", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q87_paraphrase", "query": "Give me the Intuition.", "gold_chunk_id": "6_Vector_Apr18_2021_s30_c124", "gold_span": "Intuition: why vectors? Intuition: why vectors? Consider sentiment analysis: With words, a feature is a word identity Feature 5: 'The previous word was \"terrible\"' requires exact same word to be in training and test With embeddings: Feature is a word vector 'The previous word was vector [35,22,17…] Now in the test set we might see a similar vector [34,21,14] We can generalize to similar but unseen words!!!", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q88_extract", "query": "What is From now on?", "gold_chunk_id": "6_Vector_Apr18_2021_s32_c126", "gold_span": "From now on: Computing with meaning representations instead of string representations From now on: Computing with meaning representations instead of string representations", "category": "extractive", "doc_version": "v1"}
{"qid": "q89_paraphrase", "query": "What is From now on?", "gold_chunk_id": "6_Vector_Apr18_2021_s32_c126", "gold_span": "From now on: Computing with meaning representations instead of string representations From now on: Computing with meaning representations instead of string representations", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q90_paraphrase", "query": "Give me the From now on.", "gold_chunk_id": "6_Vector_Apr18_2021_s32_c126", "gold_span": "From now on: Computing with meaning representations instead of string representations From now on: Computing with meaning representations instead of string representations", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q91_extract", "query": "What is Idea for word meaning?", "gold_chunk_id": "6_Vector_Apr18_2021_s38_c132", "gold_span": "Idea for word meaning: Words can be vectors too!!! Idea for word meaning: Words can be vectors too!!! battle is \"the kind of word that occurs in Julius Caesar and Henry V\" fool is \"the kind of word that occurs in comedies, especially Twelfth Night\" Here's the new idea: vector semantics can also be used to represent the meaning of words. We do this by associating each word with a word vector— a row vector rather than a column vector, hence with different dimensions, as we see here. The four dimensions of the vector for fool, [36,58,1,4], correspond to the four Shakespeare plays. For documents, we saw that similar documents had similar vectors, because similar documents tend to have similar words. This same principle applies to words: similar words have similar vectors because they tend to occur in similar documents. The term-document matrix thus lets us represent the meaning of a word by the documents it tends to occur in.", "category": "extractive", "doc_version": "v1"}
{"qid": "q92_paraphrase", "query": "What is Idea for word meaning?", "gold_chunk_id": "6_Vector_Apr18_2021_s38_c132", "gold_span": "Idea for word meaning: Words can be vectors too!!! Idea for word meaning: Words can be vectors too!!! battle is \"the kind of word that occurs in Julius Caesar and Henry V\" fool is \"the kind of word that occurs in comedies, especially Twelfth Night\" Here's the new idea: vector semantics can also be used to represent the meaning of words. We do this by associating each word with a word vector— a row vector rather than a column vector, hence with different dimensions, as we see here. The four dimensions of the vector for fool, [36,58,1,4], correspond to the four Shakespeare plays. For documents, we saw that similar documents had similar vectors, because similar documents tend to have similar words. This same principle applies to words: similar words have similar vectors because they tend to occur in similar documents. The term-document matrix thus lets us represent the meaning of a word by the documents it tends to occur in.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q93_paraphrase", "query": "Give me the Idea for word meaning.", "gold_chunk_id": "6_Vector_Apr18_2021_s38_c132", "gold_span": "Idea for word meaning: Words can be vectors too!!! Idea for word meaning: Words can be vectors too!!! battle is \"the kind of word that occurs in Julius Caesar and Henry V\" fool is \"the kind of word that occurs in comedies, especially Twelfth Night\" Here's the new idea: vector semantics can also be used to represent the meaning of words. We do this by associating each word with a word vector— a row vector rather than a column vector, hence with different dimensions, as we see here. The four dimensions of the vector for fool, [36,58,1,4], correspond to the four Shakespeare plays. For documents, we saw that similar documents had similar vectors, because similar documents tend to have similar words. This same principle applies to words: similar words have similar vectors because they tend to occur in similar documents. The term-document matrix thus lets us represent the meaning of a word by the documents it tends to occur in.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q94_extract", "query": "What is More common?", "gold_chunk_id": "6_Vector_Apr18_2021_s39_c133", "gold_span": "More common: word-word matrix (or \"term-context matrix\") More common: word-word matrix (or \"term-context matrix\") Two words are similar in meaning if their context vectors are similar 39 An alternative to using the term-document matrix to represent words as vectors of document counts, is to use the term-term matrix, also called the word-word matrix or the term-context matrix, in which the columns are labeled by words rather than documents. This matrix is thus of dimensionality |V | × |V | and each cell records the number of times the row (target) word and the column (context) word co-occur in some context in some training corpus. The context could be the document, in which case the cell represents the number of times the two words appear in the same document. It is most common, however, to use smaller contexts, generally a window around the word, for example of 4 words to the left and 4 words to the right, in which case the cell represents the number of times (in some training corpus) the column word occurs in such a ±4 word window around the row word. For example here is one example each of some words in their windows with counts from a Wikipedia corpus. Note that digital and information are more similar to each other than, say, to strawberry.", "category": "extractive", "doc_version": "v1"}
{"qid": "q95_paraphrase", "query": "Can you tell me about the More common?", "gold_chunk_id": "6_Vector_Apr18_2021_s39_c133", "gold_span": "More common: word-word matrix (or \"term-context matrix\") More common: word-word matrix (or \"term-context matrix\") Two words are similar in meaning if their context vectors are similar 39 An alternative to using the term-document matrix to represent words as vectors of document counts, is to use the term-term matrix, also called the word-word matrix or the term-context matrix, in which the columns are labeled by words rather than documents. This matrix is thus of dimensionality |V | × |V | and each cell records the number of times the row (target) word and the column (context) word co-occur in some context in some training corpus. The context could be the document, in which case the cell represents the number of times the two words appear in the same document. It is most common, however, to use smaller contexts, generally a window around the word, for example of 4 words to the left and 4 words to the right, in which case the cell represents the number of times (in some training corpus) the column word occurs in such a ±4 word window around the row word. For example here is one example each of some words in their windows with counts from a Wikipedia corpus. Note that digital and information are more similar to each other than, say, to strawberry.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q96_paraphrase", "query": "Please provide details about More common.", "gold_chunk_id": "6_Vector_Apr18_2021_s39_c133", "gold_span": "More common: word-word matrix (or \"term-context matrix\") More common: word-word matrix (or \"term-context matrix\") Two words are similar in meaning if their context vectors are similar 39 An alternative to using the term-document matrix to represent words as vectors of document counts, is to use the term-term matrix, also called the word-word matrix or the term-context matrix, in which the columns are labeled by words rather than documents. This matrix is thus of dimensionality |V | × |V | and each cell records the number of times the row (target) word and the column (context) word co-occur in some context in some training corpus. The context could be the document, in which case the cell represents the number of times the two words appear in the same document. It is most common, however, to use smaller contexts, generally a window around the word, for example of 4 words to the left and 4 words to the right, in which case the cell represents the number of times (in some training corpus) the column word occurs in such a ±4 word window around the row word. For example here is one example each of some words in their windows with counts from a Wikipedia corpus. Note that digital and information are more similar to each other than, say, to strawberry.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q97_extract", "query": "What is Computing word similarity?", "gold_chunk_id": "6_Vector_Apr18_2021_s43_c137", "gold_span": "Computing word similarity: Dot product and cosine Computing word similarity: Dot product and cosine The dot product between two vectors is a scalar: The dot product tends to be high when the two vectors have large values in the same dimensions Dot product can thus be a useful similarity metric between vectors The cosine—like most measures for vector similarity used in NLP—is based on the dot product operator from linear algebra, also called the inner product in which we multiply the vectors elementwise and add up to get a scalar value. The dot product acts as a similarity metric because it will tend to be high just when the two vectors have large values in the same dimensions. Alternatively, vectors that have zeros in different dimensions—orthogonal vectors—will have a dot product of 0, representing their strong dissimilarity.", "category": "extractive", "doc_version": "v1"}
{"qid": "q98_paraphrase", "query": "What is Computing word similarity?", "gold_chunk_id": "6_Vector_Apr18_2021_s43_c137", "gold_span": "Computing word similarity: Dot product and cosine Computing word similarity: Dot product and cosine The dot product between two vectors is a scalar: The dot product tends to be high when the two vectors have large values in the same dimensions Dot product can thus be a useful similarity metric between vectors The cosine—like most measures for vector similarity used in NLP—is based on the dot product operator from linear algebra, also called the inner product in which we multiply the vectors elementwise and add up to get a scalar value. The dot product acts as a similarity metric because it will tend to be high just when the two vectors have large values in the same dimensions. Alternatively, vectors that have zeros in different dimensions—orthogonal vectors—will have a dot product of 0, representing their strong dissimilarity.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q99_paraphrase", "query": "Give me the Computing word similarity.", "gold_chunk_id": "6_Vector_Apr18_2021_s43_c137", "gold_span": "Computing word similarity: Dot product and cosine Computing word similarity: Dot product and cosine The dot product between two vectors is a scalar: The dot product tends to be high when the two vectors have large values in the same dimensions Dot product can thus be a useful similarity metric between vectors The cosine—like most measures for vector similarity used in NLP—is based on the dot product operator from linear algebra, also called the inner product in which we multiply the vectors elementwise and add up to get a scalar value. The dot product acts as a similarity metric because it will tend to be high just when the two vectors have large values in the same dimensions. Alternatively, vectors that have zeros in different dimensions—orthogonal vectors—will have a dot product of 0, representing their strong dissimilarity.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q100_extract", "query": "What is Alternative?", "gold_chunk_id": "6_Vector_Apr18_2021_s45_c139", "gold_span": "Alternative: cosine for computing word similarity Alternative: cosine for computing word similarity Based on the definition of the dot product between two vectors a and b We modify the dot product to normalize for the vector length by dividing the dot product by the lengths of each of the two vectors. This normalized dot product turns out to be the same as the cosine of the angle between the two vectors (Geometrically, it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them).", "category": "extractive", "doc_version": "v1"}
{"qid": "q101_paraphrase", "query": "What is Alternative?", "gold_chunk_id": "6_Vector_Apr18_2021_s45_c139", "gold_span": "Alternative: cosine for computing word similarity Alternative: cosine for computing word similarity Based on the definition of the dot product between two vectors a and b We modify the dot product to normalize for the vector length by dividing the dot product by the lengths of each of the two vectors. This normalized dot product turns out to be the same as the cosine of the angle between the two vectors (Geometrically, it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them).", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q102_paraphrase", "query": "Summarize the Alternative.", "gold_chunk_id": "6_Vector_Apr18_2021_s45_c139", "gold_span": "Alternative: cosine for computing word similarity Alternative: cosine for computing word similarity Based on the definition of the dot product between two vectors a and b We modify the dot product to normalize for the vector length by dividing the dot product by the lengths of each of the two vectors. This normalized dot product turns out to be the same as the cosine of the angle between the two vectors (Geometrically, it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them).", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q103_extract", "query": "What is Cosine as a similarity metric Cosine as a similarity metric -1?", "gold_chunk_id": "6_Vector_Apr18_2021_s46_c140", "gold_span": "Cosine as a similarity metric Cosine as a similarity metric -1: vectors point in opposite directions +1: vectors point in same directions 0: vectors are orthogonal But since raw frequency values are non-negative, the cosine for term-term matrix vectors ranges from 0–1 46 The cosine value ranges from 1 for vectors pointing in the same direction, through 0 for orthogonal vectors, to -1 for vectors pointing in opposite directions. But since raw frequency values are non-negative, the cosine for these vectors ranges from 0–1.", "category": "extractive", "doc_version": "v1"}
{"qid": "q104_paraphrase", "query": "Can you tell me about the Cosine as a similarity metric Cosine as a similarity metric -1?", "gold_chunk_id": "6_Vector_Apr18_2021_s46_c140", "gold_span": "Cosine as a similarity metric Cosine as a similarity metric -1: vectors point in opposite directions +1: vectors point in same directions 0: vectors are orthogonal But since raw frequency values are non-negative, the cosine for term-term matrix vectors ranges from 0–1 46 The cosine value ranges from 1 for vectors pointing in the same direction, through 0 for orthogonal vectors, to -1 for vectors pointing in opposite directions. But since raw frequency values are non-negative, the cosine for these vectors ranges from 0–1.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q105_paraphrase", "query": "Give me the Cosine as a similarity metric Cosine as a similarity metric -1.", "gold_chunk_id": "6_Vector_Apr18_2021_s46_c140", "gold_span": "Cosine as a similarity metric Cosine as a similarity metric -1: vectors point in opposite directions +1: vectors point in same directions 0: vectors are orthogonal But since raw frequency values are non-negative, the cosine for term-term matrix vectors ranges from 0–1 46 The cosine value ranges from 1 for vectors pointing in the same direction, through 0 for orthogonal vectors, to -1 for vectors pointing in opposite directions. But since raw frequency values are non-negative, the cosine for these vectors ranges from 0–1.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q106_extract", "query": "What is Weighting PMI?", "gold_chunk_id": "6_Vector_Apr18_2021_s74_c168", "gold_span": "Weighting PMI: Giving rare context words slightly higher probability Weighting PMI: Giving rare context words slightly higher probability 74", "category": "extractive", "doc_version": "v1"}
{"qid": "q107_paraphrase", "query": "Please provide details about Weighting PMI.", "gold_chunk_id": "6_Vector_Apr18_2021_s74_c168", "gold_span": "Weighting PMI: Giving rare context words slightly higher probability Weighting PMI: Giving rare context words slightly higher probability 74", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q108_paraphrase", "query": "Can you tell me about the Weighting PMI?", "gold_chunk_id": "6_Vector_Apr18_2021_s74_c168", "gold_span": "Weighting PMI: Giving rare context words slightly higher probability Weighting PMI: Giving rare context words slightly higher probability 74", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q109_extract", "query": "What is CS 593 Natural Language Processing Fall 2025 Instructor?", "gold_chunk_id": "01_NLP_Course_Intro_s1_c169", "gold_span": "CS 593 Natural Language Processing Fall 2025 Instructor: Jonathan Rusert", "category": "extractive", "doc_version": "v1"}
{"qid": "q110_paraphrase", "query": "Can you tell me about the CS 593 Natural Language Processing Fall 2025 Instructor?", "gold_chunk_id": "01_NLP_Course_Intro_s1_c169", "gold_span": "CS 593 Natural Language Processing Fall 2025 Instructor: Jonathan Rusert", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q111_paraphrase", "query": "What is CS 593 Natural Language Processing Fall 2025 Instructor?", "gold_chunk_id": "01_NLP_Course_Intro_s1_c169", "gold_span": "CS 593 Natural Language Processing Fall 2025 Instructor: Jonathan Rusert", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q112_extract", "query": "What is Class Information Lecture?", "gold_chunk_id": "01_NLP_Course_Intro_s6_c174", "gold_span": "Class Information Lecture: Instructor: Jonathan Rusert Office: ET 125K Office Hours: M 3 – 4, T 12 – 1, W 1 – 3 PM or by appointment Email: jrusert@pfw.edu Course Website: purdue.brightspace.com", "category": "extractive", "doc_version": "v1"}
{"qid": "q113_paraphrase", "query": "Give me the Class Information Lecture.", "gold_chunk_id": "01_NLP_Course_Intro_s6_c174", "gold_span": "Class Information Lecture: Instructor: Jonathan Rusert Office: ET 125K Office Hours: M 3 – 4, T 12 – 1, W 1 – 3 PM or by appointment Email: jrusert@pfw.edu Course Website: purdue.brightspace.com", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q114_paraphrase", "query": "Can you tell me about the Class Information Lecture?", "gold_chunk_id": "01_NLP_Course_Intro_s6_c174", "gold_span": "Class Information Lecture: Instructor: Jonathan Rusert Office: ET 125K Office Hours: M 3 – 4, T 12 – 1, W 1 – 3 PM or by appointment Email: jrusert@pfw.edu Course Website: purdue.brightspace.com", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q115_extract", "query": "What is Student Goals External?", "gold_chunk_id": "01_NLP_Course_Intro_s10_c178", "gold_span": "Student Goals External: Develop ways to approach NLP problems Strengthen ability to examine and analyze text Internal: Improve algorithmic thinking Obtain self-sense of accomplishment", "category": "extractive", "doc_version": "v1"}
{"qid": "q116_paraphrase", "query": "Summarize the Student Goals External.", "gold_chunk_id": "01_NLP_Course_Intro_s10_c178", "gold_span": "Student Goals External: Develop ways to approach NLP problems Strengthen ability to examine and analyze text Internal: Improve algorithmic thinking Obtain self-sense of accomplishment", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q117_paraphrase", "query": "What is Student Goals External?", "gold_chunk_id": "01_NLP_Course_Intro_s10_c178", "gold_span": "Student Goals External: Develop ways to approach NLP problems Strengthen ability to examine and analyze text Internal: Improve algorithmic thinking Obtain self-sense of accomplishment", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q118_extract", "query": "What is Textbook https?", "gold_chunk_id": "01_NLP_Course_Intro_s11_c179", "gold_span": "Textbook https: //web.stanford.edu/~jurafsky/slp3/", "category": "extractive", "doc_version": "v1"}
{"qid": "q119_paraphrase", "query": "What is Textbook https?", "gold_chunk_id": "01_NLP_Course_Intro_s11_c179", "gold_span": "Textbook https: //web.stanford.edu/~jurafsky/slp3/", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q120_paraphrase", "query": "Summarize the Textbook https.", "gold_chunk_id": "01_NLP_Course_Intro_s11_c179", "gold_span": "Textbook https: //web.stanford.edu/~jurafsky/slp3/", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q121_extract", "query": "What is Corpora vary along dimension like Corpora vary along dimension like Language?", "gold_chunk_id": "02_TextPreProcessing_s8_c198", "gold_span": "Corpora vary along dimension like Corpora vary along dimension like Language: 7097 languages in the world Variety, like African American Language varieties. AAE Twitter posts might include forms like \"iont\" (I don't) Code switching, e.g., Spanish/English, Hindi/English: S/E: Por primera vez veo a @username actually being hateful! It was beautiful:) [For the first time I get to see @username actually being hateful! it was beautiful:) ] H/E: dost tha or ra- hega ... dont wory ... but dherya rakhe [“he was and will remain a friend ... don’t worry ... but have faith”] Genre: newswire, fiction, scientific articles, Wikipedia Author Demographics: writer's age, gender, ethnicity, SES For example, what language is the text in? As we'll see when we get to talking about word tokenization, what counts as a word can be different in different languages.", "category": "extractive", "doc_version": "v1"}
{"qid": "q122_paraphrase", "query": "Summarize the Corpora vary along dimension like Corpora vary along dimension like Language.", "gold_chunk_id": "02_TextPreProcessing_s8_c198", "gold_span": "Corpora vary along dimension like Corpora vary along dimension like Language: 7097 languages in the world Variety, like African American Language varieties. AAE Twitter posts might include forms like \"iont\" (I don't) Code switching, e.g., Spanish/English, Hindi/English: S/E: Por primera vez veo a @username actually being hateful! It was beautiful:) [For the first time I get to see @username actually being hateful! it was beautiful:) ] H/E: dost tha or ra- hega ... dont wory ... but dherya rakhe [“he was and will remain a friend ... don’t worry ... but have faith”] Genre: newswire, fiction, scientific articles, Wikipedia Author Demographics: writer's age, gender, ethnicity, SES For example, what language is the text in? As we'll see when we get to talking about word tokenization, what counts as a word can be different in different languages.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q123_paraphrase", "query": "Give me the Corpora vary along dimension like Corpora vary along dimension like Language.", "gold_chunk_id": "02_TextPreProcessing_s8_c198", "gold_span": "Corpora vary along dimension like Corpora vary along dimension like Language: 7097 languages in the world Variety, like African American Language varieties. AAE Twitter posts might include forms like \"iont\" (I don't) Code switching, e.g., Spanish/English, Hindi/English: S/E: Por primera vez veo a @username actually being hateful! It was beautiful:) [For the first time I get to see @username actually being hateful! it was beautiful:) ] H/E: dost tha or ra- hega ... dont wory ... but dherya rakhe [“he was and will remain a friend ... don’t worry ... but have faith”] Genre: newswire, fiction, scientific articles, Wikipedia Author Demographics: writer's age, gender, ethnicity, SES For example, what language is the text in? As we'll see when we get to talking about word tokenization, what counts as a word can be different in different languages.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q124_extract", "query": "What is Corpus datasheets Corpus datasheets Motivation?", "gold_chunk_id": "02_TextPreProcessing_s9_c199", "gold_span": "Corpus datasheets Corpus datasheets Motivation: Why was the corpus collected? By whom? Who funded it? Situation: In what situation was the text written? Collection process: If it is a subsample how was it sampled? Was there consent? Pre-processing? +Annotation process, language variety, demographics, etc. Gebru et al (2020), Bender and Friedman (2018) You'll need to make sure you consider all those properties of a text when you are using it for processing purposes. And there are even more properties of corpora that it's important to consider. Who collected this corpus? Whenever you build a corpus, you should be documenting these decisions in a datasheet for the corpus.", "category": "extractive", "doc_version": "v1"}
{"qid": "q125_paraphrase", "query": "Give me the Corpus datasheets Corpus datasheets Motivation.", "gold_chunk_id": "02_TextPreProcessing_s9_c199", "gold_span": "Corpus datasheets Corpus datasheets Motivation: Why was the corpus collected? By whom? Who funded it? Situation: In what situation was the text written? Collection process: If it is a subsample how was it sampled? Was there consent? Pre-processing? +Annotation process, language variety, demographics, etc. Gebru et al (2020), Bender and Friedman (2018) You'll need to make sure you consider all those properties of a text when you are using it for processing purposes. And there are even more properties of corpora that it's important to consider. Who collected this corpus? Whenever you build a corpus, you should be documenting these decisions in a datasheet for the corpus.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q126_paraphrase", "query": "Please provide details about Corpus datasheets Corpus datasheets Motivation.", "gold_chunk_id": "02_TextPreProcessing_s9_c199", "gold_span": "Corpus datasheets Corpus datasheets Motivation: Why was the corpus collected? By whom? Who funded it? Situation: In what situation was the text written? Collection process: If it is a subsample how was it sampled? Was there consent? Pre-processing? +Annotation process, language variety, demographics, etc. Gebru et al (2020), Bender and Friedman (2018) You'll need to make sure you consider all those properties of a text when you are using it for processing purposes. And there are even more properties of corpora that it's important to consider. Who collected this corpus? Whenever you build a corpus, you should be documenting these decisions in a datasheet for the corpus.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q127_extract", "query": "What is Text Normalization Every NLP task requires text normalization?", "gold_chunk_id": "02_TextPreProcessing_s13_c203", "gold_span": "Text Normalization Every NLP task requires text normalization: Tokenizing (segmenting) words Normalizing word formats Segmenting sentences", "category": "extractive", "doc_version": "v1"}
{"qid": "q128_paraphrase", "query": "Can you tell me about the Text Normalization Every NLP task requires text normalization?", "gold_chunk_id": "02_TextPreProcessing_s13_c203", "gold_span": "Text Normalization Every NLP task requires text normalization: Tokenizing (segmenting) words Normalizing word formats Segmenting sentences", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q129_paraphrase", "query": "Please provide details about Text Normalization Every NLP task requires text normalization.", "gold_chunk_id": "02_TextPreProcessing_s13_c203", "gold_span": "Text Normalization Every NLP task requires text normalization: Tokenizing (segmenting) words Normalizing word formats Segmenting sentences", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q130_extract", "query": "What is The first step?", "gold_chunk_id": "02_TextPreProcessing_s16_c206", "gold_span": "The first step: tokenizing The first step: tokenizing tr -sc ’A-Za-z’ ’\\n’ < shakes.txt | head THE SONNETS by William Shakespeare From fairest creatures We ...", "category": "extractive", "doc_version": "v1"}
{"qid": "q131_paraphrase", "query": "Please provide details about The first step.", "gold_chunk_id": "02_TextPreProcessing_s16_c206", "gold_span": "The first step: tokenizing The first step: tokenizing tr -sc ’A-Za-z’ ’\\n’ < shakes.txt | head THE SONNETS by William Shakespeare From fairest creatures We ...", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q132_paraphrase", "query": "What is The first step?", "gold_chunk_id": "02_TextPreProcessing_s16_c206", "gold_span": "The first step: tokenizing The first step: tokenizing tr -sc ’A-Za-z’ ’\\n’ < shakes.txt | head THE SONNETS by William Shakespeare From fairest creatures We ...", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q133_extract", "query": "What is The second step?", "gold_chunk_id": "02_TextPreProcessing_s17_c207", "gold_span": "The second step: sorting The second step: sorting tr -sc ’A-Za-z’ ’\\n’ < shakes.txt | sort | head A A A A A A A A A ...", "category": "extractive", "doc_version": "v1"}
{"qid": "q134_paraphrase", "query": "Summarize the The second step.", "gold_chunk_id": "02_TextPreProcessing_s17_c207", "gold_span": "The second step: sorting The second step: sorting tr -sc ’A-Za-z’ ’\\n’ < shakes.txt | sort | head A A A A A A A A A ...", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q135_paraphrase", "query": "Can you tell me about the The second step?", "gold_chunk_id": "02_TextPreProcessing_s17_c207", "gold_span": "The second step: sorting The second step: sorting tr -sc ’A-Za-z’ ’\\n’ < shakes.txt | sort | head A A A A A A A A A ...", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q136_extract", "query": "What is Case folding Applications like IR?", "gold_chunk_id": "02_TextPreProcessing_s31_c221", "gold_span": "Case folding Applications like IR: reduce all letters to lower case Since users tend to use lower case Possible exception: upper case in mid-sentence? e.g., General Motors Fed vs. fed SAIL vs. sail For sentiment analysis, MT, Information extraction Case is helpful (US versus us is important)", "category": "extractive", "doc_version": "v1"}
{"qid": "q137_paraphrase", "query": "Give me the Case folding Applications like IR.", "gold_chunk_id": "02_TextPreProcessing_s31_c221", "gold_span": "Case folding Applications like IR: reduce all letters to lower case Since users tend to use lower case Possible exception: upper case in mid-sentence? e.g., General Motors Fed vs. fed SAIL vs. sail For sentiment analysis, MT, Information extraction Case is helpful (US versus us is important)", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q138_paraphrase", "query": "Can you tell me about the Case folding Applications like IR?", "gold_chunk_id": "02_TextPreProcessing_s31_c221", "gold_span": "Case folding Applications like IR: reduce all letters to lower case Since users tend to use lower case Possible exception: upper case in mid-sentence? e.g., General Motors Fed vs. fed SAIL vs. sail For sentiment analysis, MT, Information extraction Case is helpful (US versus us is important)", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q139_extract", "query": "What is Lemmatization is done by Morphological Parsing Morphemes?", "gold_chunk_id": "02_TextPreProcessing_s33_c223", "gold_span": "Lemmatization is done by Morphological Parsing Morphemes: The small meaningful units that make up words Stems: The core meaning-bearing units Affixes: Parts that adhere to stems, often with grammatical functions Morphological Parsers: Parse cats into two morphemes cat and s Parse Spanish amaren (‘if in the future they would love’) into morpheme amar ‘to love’, and the morphological features 3PL and future subjunctive.", "category": "extractive", "doc_version": "v1"}
{"qid": "q140_paraphrase", "query": "Can you tell me about the Lemmatization is done by Morphological Parsing Morphemes?", "gold_chunk_id": "02_TextPreProcessing_s33_c223", "gold_span": "Lemmatization is done by Morphological Parsing Morphemes: The small meaningful units that make up words Stems: The core meaning-bearing units Affixes: Parts that adhere to stems, often with grammatical functions Morphological Parsers: Parse cats into two morphemes cat and s Parse Spanish amaren (‘if in the future they would love’) into morpheme amar ‘to love’, and the morphological features 3PL and future subjunctive.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q141_paraphrase", "query": "Summarize the Lemmatization is done by Morphological Parsing Morphemes.", "gold_chunk_id": "02_TextPreProcessing_s33_c223", "gold_span": "Lemmatization is done by Morphological Parsing Morphemes: The small meaningful units that make up words Stems: The core meaning-bearing units Affixes: Parts that adhere to stems, often with grammatical functions Morphological Parsers: Parse cats into two morphemes cat and s Parse Spanish amaren (‘if in the future they would love’) into morpheme amar ‘to love’, and the morphological features 3PL and future subjunctive.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q142_extract", "query": "What is Subword tokenization Subword tokenization Three common algorithms?", "gold_chunk_id": "02_TextPreProcessing_s41_c231", "gold_span": "Subword tokenization Subword tokenization Three common algorithms: Byte-Pair Encoding (BPE) (Sennrich et al., 2016) Unigram language modeling tokenization (Kudo, 2018) WordPiece (Schuster and Nakajima, 2012) All have 2 parts: A token learner that takes a raw training corpus and induces a vocabulary (a set of tokens). A token segmenter that takes a raw test sentence and tokenizes it according to that vocabulary", "category": "extractive", "doc_version": "v1"}
{"qid": "q143_paraphrase", "query": "Please provide details about Subword tokenization Subword tokenization Three common algorithms.", "gold_chunk_id": "02_TextPreProcessing_s41_c231", "gold_span": "Subword tokenization Subword tokenization Three common algorithms: Byte-Pair Encoding (BPE) (Sennrich et al., 2016) Unigram language modeling tokenization (Kudo, 2018) WordPiece (Schuster and Nakajima, 2012) All have 2 parts: A token learner that takes a raw training corpus and induces a vocabulary (a set of tokens). A token segmenter that takes a raw test sentence and tokenizes it according to that vocabulary", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q144_paraphrase", "query": "What is Subword tokenization Subword tokenization Three common algorithms?", "gold_chunk_id": "02_TextPreProcessing_s41_c231", "gold_span": "Subword tokenization Subword tokenization Three common algorithms: Byte-Pair Encoding (BPE) (Sennrich et al., 2016) Unigram language modeling tokenization (Kudo, 2018) WordPiece (Schuster and Nakajima, 2012) All have 2 parts: A token learner that takes a raw training corpus and induces a vocabulary (a set of tokens). A token segmenter that takes a raw test sentence and tokenizes it according to that vocabulary", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q145_extract", "query": "What is Regular Expressions?", "gold_chunk_id": "02_TextPreProcessing_s55_c245", "gold_span": "Regular Expressions: Disjunctions Letters inside square brackets [] Ranges [A-Z] Pattern | Matches [wW]oodchuck | Woodchuck, woodchuck [1234567890] | Any digit Pattern | Matches | [A-Z] | An upper case letter | Drenched Blossoms [a-z] | A lower case letter | my beans were impatient [0-9] | A single digit | Chapter 1: Down the Rabbit Hole", "category": "extractive", "doc_version": "v1"}
{"qid": "q146_paraphrase", "query": "Can you tell me about the Regular Expressions?", "gold_chunk_id": "02_TextPreProcessing_s55_c245", "gold_span": "Regular Expressions: Disjunctions Letters inside square brackets [] Ranges [A-Z] Pattern | Matches [wW]oodchuck | Woodchuck, woodchuck [1234567890] | Any digit Pattern | Matches | [A-Z] | An upper case letter | Drenched Blossoms [a-z] | A lower case letter | my beans were impatient [0-9] | A single digit | Chapter 1: Down the Rabbit Hole", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q147_paraphrase", "query": "Please provide details about Regular Expressions.", "gold_chunk_id": "02_TextPreProcessing_s55_c245", "gold_span": "Regular Expressions: Disjunctions Letters inside square brackets [] Ranges [A-Z] Pattern | Matches [wW]oodchuck | Woodchuck, woodchuck [1234567890] | Any digit Pattern | Matches | [A-Z] | An upper case letter | Drenched Blossoms [a-z] | A lower case letter | my beans were impatient [0-9] | A single digit | Chapter 1: Down the Rabbit Hole", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q148_extract", "query": "What is Regular Expressions?", "gold_chunk_id": "02_TextPreProcessing_s56_c246", "gold_span": "Regular Expressions: Negation in Disjunction Negations [^Ss] Carat means negation only when first in [] Pattern | Matches | [^A-Z] | Not an upper case letter | Oyfn pripetchik [^Ss] | Neither ‘S’ nor ‘s’ | I have no exquisite reason” [^e^] | Neither e nor ^ | Look here a^b | The pattern a carat b | Look up a^b now", "category": "extractive", "doc_version": "v1"}
{"qid": "q149_paraphrase", "query": "What is Regular Expressions?", "gold_chunk_id": "02_TextPreProcessing_s56_c246", "gold_span": "Regular Expressions: Negation in Disjunction Negations [^Ss] Carat means negation only when first in [] Pattern | Matches | [^A-Z] | Not an upper case letter | Oyfn pripetchik [^Ss] | Neither ‘S’ nor ‘s’ | I have no exquisite reason” [^e^] | Neither e nor ^ | Look here a^b | The pattern a carat b | Look up a^b now", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q150_paraphrase", "query": "Please provide details about Regular Expressions.", "gold_chunk_id": "02_TextPreProcessing_s56_c246", "gold_span": "Regular Expressions: Negation in Disjunction Negations [^Ss] Carat means negation only when first in [] Pattern | Matches | [^A-Z] | Not an upper case letter | Oyfn pripetchik [^Ss] | Neither ‘S’ nor ‘s’ | I have no exquisite reason” [^e^] | Neither e nor ^ | Look here a^b | The pattern a carat b | Look up a^b now", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q151_extract", "query": "What is Regular Expressions?", "gold_chunk_id": "02_TextPreProcessing_s57_c247", "gold_span": "Regular Expressions: More Disjunction Woodchuck is another name for groundhog! The pipe | for disjunction Pattern | Matches groundhog|woodchuck | woodchuck yours|mine | yours a|b|c | = [abc] [gG]roundhog|[Ww]oodchuck | Woodchuck", "category": "extractive", "doc_version": "v1"}
{"qid": "q152_paraphrase", "query": "Give me the Regular Expressions.", "gold_chunk_id": "02_TextPreProcessing_s57_c247", "gold_span": "Regular Expressions: More Disjunction Woodchuck is another name for groundhog! The pipe | for disjunction Pattern | Matches groundhog|woodchuck | woodchuck yours|mine | yours a|b|c | = [abc] [gG]roundhog|[Ww]oodchuck | Woodchuck", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q153_paraphrase", "query": "Summarize the Regular Expressions.", "gold_chunk_id": "02_TextPreProcessing_s57_c247", "gold_span": "Regular Expressions: More Disjunction Woodchuck is another name for groundhog! The pipe | for disjunction Pattern | Matches groundhog|woodchuck | woodchuck yours|mine | yours a|b|c | = [abc] [gG]roundhog|[Ww]oodchuck | Woodchuck", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q154_extract", "query": "What is Regular Expressions?", "gold_chunk_id": "02_TextPreProcessing_s58_c248", "gold_span": "Regular Expressions: ? *+. Stephen C Kleene Pattern | Matches | colou?r | Optional previous char | color colour oo*h! | 0 or more of previous char | oh! ooh! oooh! ooooh! o+h! | 1 or more of previous char | oh! ooh! oooh! ooooh! baa+ | | baa baaa baaaa baaaaa beg.n | | begin begun begun beg3n Kleene *, Kleene +", "category": "extractive", "doc_version": "v1"}
{"qid": "q155_paraphrase", "query": "What is Regular Expressions?", "gold_chunk_id": "02_TextPreProcessing_s58_c248", "gold_span": "Regular Expressions: ? *+. Stephen C Kleene Pattern | Matches | colou?r | Optional previous char | color colour oo*h! | 0 or more of previous char | oh! ooh! oooh! ooooh! o+h! | 1 or more of previous char | oh! ooh! oooh! ooooh! baa+ | | baa baaa baaaa baaaaa beg.n | | begin begun begun beg3n Kleene *, Kleene +", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q156_paraphrase", "query": "Summarize the Regular Expressions.", "gold_chunk_id": "02_TextPreProcessing_s58_c248", "gold_span": "Regular Expressions: ? *+. Stephen C Kleene Pattern | Matches | colou?r | Optional previous char | color colour oo*h! | 0 or more of previous char | oh! ooh! oooh! ooooh! o+h! | 1 or more of previous char | oh! ooh! oooh! ooooh! baa+ | | baa baaa baaaa baaaaa beg.n | | begin begun begun beg3n Kleene *, Kleene +", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q157_extract", "query": "What is Regular Expressions?", "gold_chunk_id": "02_TextPreProcessing_s59_c249", "gold_span": "Regular Expressions: Anchors ^ $ Pattern | Matches ^[A-Z] | Palo Alto ^[^A-Za-z] | 1 “Hello” \\.$ | The end. .$ | The end? The end!", "category": "extractive", "doc_version": "v1"}
{"qid": "q158_paraphrase", "query": "Please provide details about Regular Expressions.", "gold_chunk_id": "02_TextPreProcessing_s59_c249", "gold_span": "Regular Expressions: Anchors ^ $ Pattern | Matches ^[A-Z] | Palo Alto ^[^A-Za-z] | 1 “Hello” \\.$ | The end. .$ | The end? The end!", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q159_paraphrase", "query": "Can you tell me about the Regular Expressions?", "gold_chunk_id": "02_TextPreProcessing_s59_c249", "gold_span": "Regular Expressions: Anchors ^ $ Pattern | Matches ^[A-Z] | Palo Alto ^[^A-Za-z] | 1 “Hello” \\.$ | The end. .$ | The end? The end!", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q160_extract", "query": "What is Errors The process we just went through was based on fixing two kinds of errors?", "gold_chunk_id": "02_TextPreProcessing_s62_c252", "gold_span": "Errors The process we just went through was based on fixing two kinds of errors: Matching strings that we should not have matched (there, then, other) False positives (Type I errors) Not matching things that we should have matched (The) False negatives (Type II errors)", "category": "extractive", "doc_version": "v1"}
{"qid": "q161_paraphrase", "query": "What is Errors The process we just went through was based on fixing two kinds of errors?", "gold_chunk_id": "02_TextPreProcessing_s62_c252", "gold_span": "Errors The process we just went through was based on fixing two kinds of errors: Matching strings that we should not have matched (there, then, other) False positives (Type I errors) Not matching things that we should have matched (The) False negatives (Type II errors)", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q162_paraphrase", "query": "Can you tell me about the Errors The process we just went through was based on fixing two kinds of errors?", "gold_chunk_id": "02_TextPreProcessing_s62_c252", "gold_span": "Errors The process we just went through was based on fixing two kinds of errors: Matching strings that we should not have matched (there, then, other) False positives (Type I errors) Not matching things that we should have matched (The) False negatives (Type II errors)", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q163_extract", "query": "What is Basic Text Processing More Regular Expressions?", "gold_chunk_id": "02_TextPreProcessing_s66_c256", "gold_span": "Basic Text Processing More Regular Expressions: Substitutions and ELIZA Regular expression play a very powerful role when they are used to change strings, substituting one string for another. And this power to easily model string substitutions turns out to play a role in one of the earliest NLP systems, the pioneering 1966 chatbot ELIZA.", "category": "extractive", "doc_version": "v1"}
{"qid": "q164_paraphrase", "query": "Please provide details about Basic Text Processing More Regular Expressions.", "gold_chunk_id": "02_TextPreProcessing_s66_c256", "gold_span": "Basic Text Processing More Regular Expressions: Substitutions and ELIZA Regular expression play a very powerful role when they are used to change strings, substituting one string for another. And this power to easily model string substitutions turns out to play a role in one of the earliest NLP systems, the pioneering 1966 chatbot ELIZA.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q165_paraphrase", "query": "Can you tell me about the Basic Text Processing More Regular Expressions?", "gold_chunk_id": "02_TextPreProcessing_s66_c256", "gold_span": "Basic Text Processing More Regular Expressions: Substitutions and ELIZA Regular expression play a very powerful role when they are used to change strings, substituting one string for another. And this power to easily model string substitutions turns out to play a role in one of the earliest NLP systems, the pioneering 1966 chatbot ELIZA.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q166_extract", "query": "What is Substitutions Substitutions Substitution in Python and UNIX commands?", "gold_chunk_id": "02_TextPreProcessing_s67_c257", "gold_span": "Substitutions Substitutions Substitution in Python and UNIX commands: s/regexp1/pattern/ e.g.: s/colour/color/ The syntax for substitutions is simple. For example the python \"S\" command can be used to change a string matched by a regex to the substitute, another string.", "category": "extractive", "doc_version": "v1"}
{"qid": "q167_paraphrase", "query": "Summarize the Substitutions Substitutions Substitution in Python and UNIX commands.", "gold_chunk_id": "02_TextPreProcessing_s67_c257", "gold_span": "Substitutions Substitutions Substitution in Python and UNIX commands: s/regexp1/pattern/ e.g.: s/colour/color/ The syntax for substitutions is simple. For example the python \"S\" command can be used to change a string matched by a regex to the substitute, another string.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q168_paraphrase", "query": "What is Substitutions Substitutions Substitution in Python and UNIX commands?", "gold_chunk_id": "02_TextPreProcessing_s67_c257", "gold_span": "Substitutions Substitutions Substitution in Python and UNIX commands: s/regexp1/pattern/ e.g.: s/colour/color/ The syntax for substitutions is simple. For example the python \"S\" command can be used to change a string matched by a regex to the substitute, another string.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q169_extract", "query": "What is Capture Groups Capture Groups Say we want to put angles around all numbers?", "gold_chunk_id": "02_TextPreProcessing_s68_c258", "gold_span": "Capture Groups Capture Groups Say we want to put angles around all numbers: the 35 boxes  the <35> boxes Use parens () to \"capture\" a pattern into a numbered register (1, 2, 3…) Use \\1 to refer to the contents of the register s/([0-9]+)/<\\1>/ (note on regex101 \\1 = $1 for substitution) It is often useful to be able to refer to a particular subpart of the string matching the first pattern. For that, we can use \"capture groups\", a way of storing part of the pattern into a \"register\" so we can refer to it later in the substitution string.", "category": "extractive", "doc_version": "v1"}
{"qid": "q170_paraphrase", "query": "Give me the Capture Groups Capture Groups Say we want to put angles around all numbers.", "gold_chunk_id": "02_TextPreProcessing_s68_c258", "gold_span": "Capture Groups Capture Groups Say we want to put angles around all numbers: the 35 boxes  the <35> boxes Use parens () to \"capture\" a pattern into a numbered register (1, 2, 3…) Use \\1 to refer to the contents of the register s/([0-9]+)/<\\1>/ (note on regex101 \\1 = $1 for substitution) It is often useful to be able to refer to a particular subpart of the string matching the first pattern. For that, we can use \"capture groups\", a way of storing part of the pattern into a \"register\" so we can refer to it later in the substitution string.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q171_paraphrase", "query": "Please provide details about Capture Groups Capture Groups Say we want to put angles around all numbers.", "gold_chunk_id": "02_TextPreProcessing_s68_c258", "gold_span": "Capture Groups Capture Groups Say we want to put angles around all numbers: the 35 boxes  the <35> boxes Use parens () to \"capture\" a pattern into a numbered register (1, 2, 3…) Use \\1 to refer to the contents of the register s/([0-9]+)/<\\1>/ (note on regex101 \\1 = $1 for substitution) It is often useful to be able to refer to a particular subpart of the string matching the first pattern. For that, we can use \"capture groups\", a way of storing part of the pattern into a \"register\" so we can refer to it later in the substitution string.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q172_extract", "query": "What is Capture groups?", "gold_chunk_id": "02_TextPreProcessing_s69_c259", "gold_span": "Capture groups: multiple registers Capture groups: multiple registers /the (.*)er they (.*), the \\1er we \\2/ Matches the faster they ran, the faster we ran But not the faster they ran, the faster we ate In very complex patterns, we'll want to use more than one register; here's an example where we first capture two strings, and then refer to them both in order.", "category": "extractive", "doc_version": "v1"}
{"qid": "q173_paraphrase", "query": "Give me the Capture groups.", "gold_chunk_id": "02_TextPreProcessing_s69_c259", "gold_span": "Capture groups: multiple registers Capture groups: multiple registers /the (.*)er they (.*), the \\1er we \\2/ Matches the faster they ran, the faster we ran But not the faster they ran, the faster we ate In very complex patterns, we'll want to use more than one register; here's an example where we first capture two strings, and then refer to them both in order.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q174_paraphrase", "query": "Can you tell me about the Capture groups?", "gold_chunk_id": "02_TextPreProcessing_s69_c259", "gold_span": "Capture groups: multiple registers Capture groups: multiple registers /the (.*)er they (.*), the \\1er we \\2/ Matches the faster they ran, the faster we ran But not the faster they ran, the faster we ate In very complex patterns, we'll want to use more than one register; here's an example where we first capture two strings, and then refer to them both in order.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q175_extract", "query": "What is Simple Application?", "gold_chunk_id": "02_TextPreProcessing_s72_c262", "gold_span": "Simple Application: ELIZA Simple Application: ELIZA Early NLP system that imitated a Rogerian psychotherapist Joseph Weizenbaum, 1966. Uses pattern matching to match, e.g.,: “I need X” and translates them into, e.g. “What would it mean to you if you got X? Substitutions and capture groups are very useful in implementing simple chat-bots like ELIZA. ELIZA , one of the most important historical NLP systems, was created by pioneering AI researcher Joseph Weizenbaum in 1966, and simulates a Rogerian psychologist, a kind of therapist who emphasizes mirroring back what they hear. ELIZA is a surprisingly simple program that uses pattern matching to recognize phrases like “I need X” and translate them into suitable outputs like “What would it mean to you if you got X?”. This simple technique succeeds in this domain because ELIZA doesn’t actually need to know anything to mimic a Rogerian psychotherapist. This is one of the few dialogue genres where listeners can act as if they know nothing of the world.", "category": "extractive", "doc_version": "v1"}
{"qid": "q176_paraphrase", "query": "Please provide details about Simple Application.", "gold_chunk_id": "02_TextPreProcessing_s72_c262", "gold_span": "Simple Application: ELIZA Simple Application: ELIZA Early NLP system that imitated a Rogerian psychotherapist Joseph Weizenbaum, 1966. Uses pattern matching to match, e.g.,: “I need X” and translates them into, e.g. “What would it mean to you if you got X? Substitutions and capture groups are very useful in implementing simple chat-bots like ELIZA. ELIZA , one of the most important historical NLP systems, was created by pioneering AI researcher Joseph Weizenbaum in 1966, and simulates a Rogerian psychologist, a kind of therapist who emphasizes mirroring back what they hear. ELIZA is a surprisingly simple program that uses pattern matching to recognize phrases like “I need X” and translate them into suitable outputs like “What would it mean to you if you got X?”. This simple technique succeeds in this domain because ELIZA doesn’t actually need to know anything to mimic a Rogerian psychotherapist. This is one of the few dialogue genres where listeners can act as if they know nothing of the world.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q177_paraphrase", "query": "Summarize the Simple Application.", "gold_chunk_id": "02_TextPreProcessing_s72_c262", "gold_span": "Simple Application: ELIZA Simple Application: ELIZA Early NLP system that imitated a Rogerian psychotherapist Joseph Weizenbaum, 1966. Uses pattern matching to match, e.g.,: “I need X” and translates them into, e.g. “What would it mean to you if you got X? Substitutions and capture groups are very useful in implementing simple chat-bots like ELIZA. ELIZA , one of the most important historical NLP systems, was created by pioneering AI researcher Joseph Weizenbaum in 1966, and simulates a Rogerian psychologist, a kind of therapist who emphasizes mirroring back what they hear. ELIZA is a surprisingly simple program that uses pattern matching to recognize phrases like “I need X” and translate them into suitable outputs like “What would it mean to you if you got X?”. This simple technique succeeds in this domain because ELIZA doesn’t actually need to know anything to mimic a Rogerian psychotherapist. This is one of the few dialogue genres where listeners can act as if they know nothing of the world.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q178_extract", "query": "What is Simple Application?", "gold_chunk_id": "02_TextPreProcessing_s73_c263", "gold_span": "Simple Application: ELIZA Simple Application: ELIZA Men are all alike. IN WHAT WAY They're always bugging us about something or other. CAN YOU THINK OF A SPECIFIC EXAMPLE Well, my boyfriend made me come here. YOUR BOYFRIEND MADE YOU COME HERE He says I'm depressed much of the time. I AM SORRY TO HEAR YOU ARE DEPRESSED Here's some fragments from a sample conversation with ELIZA in 1966. Eliza’s mimicry of human conversation was remarkably successful: many people who interacted with ELIZA came to believe that it really understood them and their problems, and in very prescient early work, Weizenbaum pointed out the ethical issues in this attribution of human qualities to an artificial agent. We'll return to this issue in the dialogue lectures.", "category": "extractive", "doc_version": "v1"}
{"qid": "q179_paraphrase", "query": "Give me the Simple Application.", "gold_chunk_id": "02_TextPreProcessing_s73_c263", "gold_span": "Simple Application: ELIZA Simple Application: ELIZA Men are all alike. IN WHAT WAY They're always bugging us about something or other. CAN YOU THINK OF A SPECIFIC EXAMPLE Well, my boyfriend made me come here. YOUR BOYFRIEND MADE YOU COME HERE He says I'm depressed much of the time. I AM SORRY TO HEAR YOU ARE DEPRESSED Here's some fragments from a sample conversation with ELIZA in 1966. Eliza’s mimicry of human conversation was remarkably successful: many people who interacted with ELIZA came to believe that it really understood them and their problems, and in very prescient early work, Weizenbaum pointed out the ethical issues in this attribution of human qualities to an artificial agent. We'll return to this issue in the dialogue lectures.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q180_paraphrase", "query": "What is Simple Application?", "gold_chunk_id": "02_TextPreProcessing_s73_c263", "gold_span": "Simple Application: ELIZA Simple Application: ELIZA Men are all alike. IN WHAT WAY They're always bugging us about something or other. CAN YOU THINK OF A SPECIFIC EXAMPLE Well, my boyfriend made me come here. YOUR BOYFRIEND MADE YOU COME HERE He says I'm depressed much of the time. I AM SORRY TO HEAR YOU ARE DEPRESSED Here's some fragments from a sample conversation with ELIZA in 1966. Eliza’s mimicry of human conversation was remarkably successful: many people who interacted with ELIZA came to believe that it really understood them and their problems, and in very prescient early work, Weizenbaum pointed out the ethical issues in this attribution of human qualities to an artificial agent. We'll return to this issue in the dialogue lectures.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q181_extract", "query": "What is Basic Text Processing More Regular Expressions?", "gold_chunk_id": "02_TextPreProcessing_s75_c265", "gold_span": "Basic Text Processing More Regular Expressions: Substitutions and ELIZA You'll find regular expression substitutions, and more powerful tools like lookahead, to be useful in all sorts of applications. And later on we'll be returning to ELIZA and the general issue of building agents that can interact conversationally.", "category": "extractive", "doc_version": "v1"}
{"qid": "q182_paraphrase", "query": "What is Basic Text Processing More Regular Expressions?", "gold_chunk_id": "02_TextPreProcessing_s75_c265", "gold_span": "Basic Text Processing More Regular Expressions: Substitutions and ELIZA You'll find regular expression substitutions, and more powerful tools like lookahead, to be useful in all sorts of applications. And later on we'll be returning to ELIZA and the general issue of building agents that can interact conversationally.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q183_paraphrase", "query": "Can you tell me about the Basic Text Processing More Regular Expressions?", "gold_chunk_id": "02_TextPreProcessing_s75_c265", "gold_span": "Basic Text Processing More Regular Expressions: Substitutions and ELIZA You'll find regular expression substitutions, and more powerful tools like lookahead, to be useful in all sorts of applications. And later on we'll be returning to ELIZA and the general issue of building agents that can interact conversationally.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q184_extract", "query": "What is Summary?", "gold_chunk_id": "4_NB_Apr_4_2021_s11_c276", "gold_span": "Summary: Text Classification Sentiment analysis Spam detection Authorship identification Language Identification Assigning subject categories, topics, or genres …", "category": "extractive", "doc_version": "v1"}
{"qid": "q185_paraphrase", "query": "What is Summary?", "gold_chunk_id": "4_NB_Apr_4_2021_s11_c276", "gold_span": "Summary: Text Classification Sentiment analysis Spam detection Authorship identification Language Identification Assigning subject categories, topics, or genres …", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q186_paraphrase", "query": "Summarize the Summary.", "gold_chunk_id": "4_NB_Apr_4_2021_s11_c276", "gold_span": "Summary: Text Classification Sentiment analysis Spam detection Authorship identification Language Identification Assigning subject categories, topics, or genres …", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q187_extract", "query": "What is Text Classification?", "gold_chunk_id": "4_NB_Apr_4_2021_s12_c277", "gold_span": "Text Classification: definition Input: a document d a fixed set of classes C = {c1, c2,…, cJ} Output: a predicted class c  C", "category": "extractive", "doc_version": "v1"}
{"qid": "q188_paraphrase", "query": "Summarize the Text Classification.", "gold_chunk_id": "4_NB_Apr_4_2021_s12_c277", "gold_span": "Text Classification: definition Input: a document d a fixed set of classes C = {c1, c2,…, cJ} Output: a predicted class c  C", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q189_paraphrase", "query": "Give me the Text Classification.", "gold_chunk_id": "4_NB_Apr_4_2021_s12_c277", "gold_span": "Text Classification: definition Input: a document d a fixed set of classes C = {c1, c2,…, cJ} Output: a predicted class c  C", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q190_extract", "query": "What is Classification Methods?", "gold_chunk_id": "4_NB_Apr_4_2021_s13_c278", "gold_span": "Classification Methods: Hand-coded rules Rules based on combinations of words or other features spam: black-list-address OR (“dollars” AND “you have been selected”) Accuracy can be high If rules carefully refined by expert But building and maintaining these rules is expensive", "category": "extractive", "doc_version": "v1"}
{"qid": "q191_paraphrase", "query": "Can you tell me about the Classification Methods?", "gold_chunk_id": "4_NB_Apr_4_2021_s13_c278", "gold_span": "Classification Methods: Hand-coded rules Rules based on combinations of words or other features spam: black-list-address OR (“dollars” AND “you have been selected”) Accuracy can be high If rules carefully refined by expert But building and maintaining these rules is expensive", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q192_paraphrase", "query": "Give me the Classification Methods.", "gold_chunk_id": "4_NB_Apr_4_2021_s13_c278", "gold_span": "Classification Methods: Hand-coded rules Rules based on combinations of words or other features spam: black-list-address OR (“dollars” AND “you have been selected”) Accuracy can be high If rules carefully refined by expert But building and maintaining these rules is expensive", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q193_extract", "query": "What is Classification Methods?", "gold_chunk_id": "4_NB_Apr_4_2021_s14_c279", "gold_span": "Classification Methods: Supervised Machine Learning Classification Methods: Supervised Machine Learning Input: a document d a fixed set of classes C = {c1, c2,…, cJ} A training set of m hand-labeled documents (d1,c1),....,(dm,cm) Output: a learned classifier γ:d  c 14", "category": "extractive", "doc_version": "v1"}
{"qid": "q194_paraphrase", "query": "Please provide details about Classification Methods.", "gold_chunk_id": "4_NB_Apr_4_2021_s14_c279", "gold_span": "Classification Methods: Supervised Machine Learning Classification Methods: Supervised Machine Learning Input: a document d a fixed set of classes C = {c1, c2,…, cJ} A training set of m hand-labeled documents (d1,c1),....,(dm,cm) Output: a learned classifier γ:d  c 14", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q195_paraphrase", "query": "What is Classification Methods?", "gold_chunk_id": "4_NB_Apr_4_2021_s14_c279", "gold_span": "Classification Methods: Supervised Machine Learning Classification Methods: Supervised Machine Learning Input: a document d a fixed set of classes C = {c1, c2,…, cJ} A training set of m hand-labeled documents (d1,c1),....,(dm,cm) Output: a learned classifier γ:d  c 14", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q196_extract", "query": "What is Classification Methods?", "gold_chunk_id": "4_NB_Apr_4_2021_s15_c280", "gold_span": "Classification Methods: Supervised Machine Learning Any kind of classifier Naïve Bayes Logistic regression Neural networks k-Nearest Neighbors …", "category": "extractive", "doc_version": "v1"}
{"qid": "q197_paraphrase", "query": "Can you tell me about the Classification Methods?", "gold_chunk_id": "4_NB_Apr_4_2021_s15_c280", "gold_span": "Classification Methods: Supervised Machine Learning Any kind of classifier Naïve Bayes Logistic regression Neural networks k-Nearest Neighbors …", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q198_paraphrase", "query": "Give me the Classification Methods.", "gold_chunk_id": "4_NB_Apr_4_2021_s15_c280", "gold_span": "Classification Methods: Supervised Machine Learning Any kind of classifier Naïve Bayes Logistic regression Neural networks k-Nearest Neighbors …", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q199_extract", "query": "What is Multinomial Naive Bayes Independence Assumptions Bag of Words assumption?", "gold_chunk_id": "4_NB_Apr_4_2021_s25_c290", "gold_span": "Multinomial Naive Bayes Independence Assumptions Bag of Words assumption: Assume position doesn’t matter Conditional Independence: Assume the feature probabilities P(xi|cj) are independent given the class c.", "category": "extractive", "doc_version": "v1"}
{"qid": "q200_paraphrase", "query": "Can you tell me about the Multinomial Naive Bayes Independence Assumptions Bag of Words assumption?", "gold_chunk_id": "4_NB_Apr_4_2021_s25_c290", "gold_span": "Multinomial Naive Bayes Independence Assumptions Bag of Words assumption: Assume position doesn’t matter Conditional Independence: Assume the feature probabilities P(xi|cj) are independent given the class c.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q201_paraphrase", "query": "Please provide details about Multinomial Naive Bayes Independence Assumptions Bag of Words assumption.", "gold_chunk_id": "4_NB_Apr_4_2021_s25_c290", "gold_span": "Multinomial Naive Bayes Independence Assumptions Bag of Words assumption: Assume position doesn’t matter Conditional Independence: Assume the feature probabilities P(xi|cj) are independent given the class c.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q202_extract", "query": "What is We actually do everything in log space Instead of this?", "gold_chunk_id": "4_NB_Apr_4_2021_s29_c294", "gold_span": "We actually do everything in log space Instead of this: This: Notes: 1) Taking log doesn't change the ranking of classes! The class with highest probability also has highest log probability! 2) It's a linear model: Just a max of a sum of weights: a linear function of the inputs So naive bayes is a linear classifier", "category": "extractive", "doc_version": "v1"}
{"qid": "q203_paraphrase", "query": "What is We actually do everything in log space Instead of this?", "gold_chunk_id": "4_NB_Apr_4_2021_s29_c294", "gold_span": "We actually do everything in log space Instead of this: This: Notes: 1) Taking log doesn't change the ranking of classes! The class with highest probability also has highest log probability! 2) It's a linear model: Just a max of a sum of weights: a linear function of the inputs So naive bayes is a linear classifier", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q204_paraphrase", "query": "Give me the We actually do everything in log space Instead of this.", "gold_chunk_id": "4_NB_Apr_4_2021_s29_c294", "gold_span": "We actually do everything in log space Instead of this: This: Notes: 1) Taking log doesn't change the ranking of classes! The class with highest probability also has highest log probability! 2) It's a linear model: Just a max of a sum of weights: a linear function of the inputs So naive bayes is a linear classifier", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q205_extract", "query": "What is Learning the Multinomial Naive Bayes Model First attempt?", "gold_chunk_id": "4_NB_Apr_4_2021_s32_c297", "gold_span": "Learning the Multinomial Naive Bayes Model First attempt: maximum likelihood estimates simply use the frequencies in the data Sec.13.3", "category": "extractive", "doc_version": "v1"}
{"qid": "q206_paraphrase", "query": "Can you tell me about the Learning the Multinomial Naive Bayes Model First attempt?", "gold_chunk_id": "4_NB_Apr_4_2021_s32_c297", "gold_span": "Learning the Multinomial Naive Bayes Model First attempt: maximum likelihood estimates simply use the frequencies in the data Sec.13.3", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q207_paraphrase", "query": "Please provide details about Learning the Multinomial Naive Bayes Model First attempt.", "gold_chunk_id": "4_NB_Apr_4_2021_s32_c297", "gold_span": "Learning the Multinomial Naive Bayes Model First attempt: maximum likelihood estimates simply use the frequencies in the data Sec.13.3", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q208_extract", "query": "What is Stop words Stop words Some systems ignore stop words Stop words?", "gold_chunk_id": "4_NB_Apr_4_2021_s38_c303", "gold_span": "Stop words Stop words Some systems ignore stop words Stop words: very frequent words like the and a. Sort the vocabulary by word frequency in training set Call the top 10 or 50 words the stopword list. Remove all stop words from both training and test sets As if they were never there! But removing stop words doesn't usually help So in practice most NB algorithms use all words and don't use stopword lists", "category": "extractive", "doc_version": "v1"}
{"qid": "q209_paraphrase", "query": "What is Stop words Stop words Some systems ignore stop words Stop words?", "gold_chunk_id": "4_NB_Apr_4_2021_s38_c303", "gold_span": "Stop words Stop words Some systems ignore stop words Stop words: very frequent words like the and a. Sort the vocabulary by word frequency in training set Call the top 10 or 50 words the stopword list. Remove all stop words from both training and test sets As if they were never there! But removing stop words doesn't usually help So in practice most NB algorithms use all words and don't use stopword lists", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q210_paraphrase", "query": "Can you tell me about the Stop words Stop words Some systems ignore stop words Stop words?", "gold_chunk_id": "4_NB_Apr_4_2021_s38_c303", "gold_span": "Stop words Stop words Some systems ignore stop words Stop words: very frequent words like the and a. Sort the vocabulary by word frequency in training set Call the top 10 or 50 words the stopword list. Remove all stop words from both training and test sets As if they were never there! But removing stop words doesn't usually help So in practice most NB algorithms use all words and don't use stopword lists", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q211_extract", "query": "What is Text Classification and Naive Bayes Naive Bayes?", "gold_chunk_id": "4_NB_Apr_4_2021_s39_c304", "gold_span": "Text Classification and Naive Bayes Naive Bayes: Learning", "category": "extractive", "doc_version": "v1"}
{"qid": "q212_paraphrase", "query": "Summarize the Text Classification and Naive Bayes Naive Bayes.", "gold_chunk_id": "4_NB_Apr_4_2021_s39_c304", "gold_span": "Text Classification and Naive Bayes Naive Bayes: Learning", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q213_paraphrase", "query": "Can you tell me about the Text Classification and Naive Bayes Naive Bayes?", "gold_chunk_id": "4_NB_Apr_4_2021_s39_c304", "gold_span": "Text Classification and Naive Bayes Naive Bayes: Learning", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q214_extract", "query": "What is Sentiment Classification?", "gold_chunk_id": "4_NB_Apr_4_2021_s52_c317", "gold_span": "Sentiment Classification: Dealing with Negation Sentiment Classification: Dealing with Negation I really like this movie I really don't like this movie Negation changes the meaning of \"like\" to negative. Negation can also change negative to positive-ish Don't dismiss this film Doesn't let us get bored", "category": "extractive", "doc_version": "v1"}
{"qid": "q215_paraphrase", "query": "Summarize the Sentiment Classification.", "gold_chunk_id": "4_NB_Apr_4_2021_s52_c317", "gold_span": "Sentiment Classification: Dealing with Negation Sentiment Classification: Dealing with Negation I really like this movie I really don't like this movie Negation changes the meaning of \"like\" to negative. Negation can also change negative to positive-ish Don't dismiss this film Doesn't let us get bored", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q216_paraphrase", "query": "Give me the Sentiment Classification.", "gold_chunk_id": "4_NB_Apr_4_2021_s52_c317", "gold_span": "Sentiment Classification: Dealing with Negation Sentiment Classification: Dealing with Negation I really like this movie I really don't like this movie Negation changes the meaning of \"like\" to negative. Negation can also change negative to positive-ish Don't dismiss this film Doesn't let us get bored", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q217_extract", "query": "What is Sentiment Classification?", "gold_chunk_id": "4_NB_Apr_4_2021_s53_c318", "gold_span": "Sentiment Classification: Dealing with Negation Sentiment Classification: Dealing with Negation Simple baseline method: Add NOT_ to every word between negation and following punctuation: didn’t like this movie , but I didn’t NOT_like NOT_this NOT_movie but I Das, Sanjiv and Mike Chen. 2001. Yahoo! for Amazon: Extracting market sentiment from stock message boards. In Proceedings of the Asia Pacific Finance Association Annual Conference (APFA). Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment Classification using Machine Learning Techniques. EMNLP-2002, 79—86.", "category": "extractive", "doc_version": "v1"}
{"qid": "q218_paraphrase", "query": "Can you tell me about the Sentiment Classification?", "gold_chunk_id": "4_NB_Apr_4_2021_s53_c318", "gold_span": "Sentiment Classification: Dealing with Negation Sentiment Classification: Dealing with Negation Simple baseline method: Add NOT_ to every word between negation and following punctuation: didn’t like this movie , but I didn’t NOT_like NOT_this NOT_movie but I Das, Sanjiv and Mike Chen. 2001. Yahoo! for Amazon: Extracting market sentiment from stock message boards. In Proceedings of the Asia Pacific Finance Association Annual Conference (APFA). Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment Classification using Machine Learning Techniques. EMNLP-2002, 79—86.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q219_paraphrase", "query": "Give me the Sentiment Classification.", "gold_chunk_id": "4_NB_Apr_4_2021_s53_c318", "gold_span": "Sentiment Classification: Dealing with Negation Sentiment Classification: Dealing with Negation Simple baseline method: Add NOT_ to every word between negation and following punctuation: didn’t like this movie , but I didn’t NOT_like NOT_this NOT_movie but I Das, Sanjiv and Mike Chen. 2001. Yahoo! for Amazon: Extracting market sentiment from stock message boards. In Proceedings of the Asia Pacific Finance Association Annual Conference (APFA). Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment Classification using Machine Learning Techniques. EMNLP-2002, 79—86.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q220_extract", "query": "What is Sentiment Classification?", "gold_chunk_id": "4_NB_Apr_4_2021_s54_c319", "gold_span": "Sentiment Classification: Lexicons Sentiment Classification: Lexicons Sometimes we don't have enough labeled training data In that case, we can make use of pre-built word lists Called lexicons There are various publically available lexicons", "category": "extractive", "doc_version": "v1"}
{"qid": "q221_paraphrase", "query": "Please provide details about Sentiment Classification.", "gold_chunk_id": "4_NB_Apr_4_2021_s54_c319", "gold_span": "Sentiment Classification: Lexicons Sentiment Classification: Lexicons Sometimes we don't have enough labeled training data In that case, we can make use of pre-built word lists Called lexicons There are various publically available lexicons", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q222_paraphrase", "query": "Can you tell me about the Sentiment Classification?", "gold_chunk_id": "4_NB_Apr_4_2021_s54_c319", "gold_span": "Sentiment Classification: Lexicons Sentiment Classification: Lexicons Sometimes we don't have enough labeled training data In that case, we can make use of pre-built word lists Called lexicons There are various publically available lexicons", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q223_extract", "query": "What is MPQA Subjectivity Cues Lexicon MPQA Subjectivity Cues Lexicon Home page?", "gold_chunk_id": "4_NB_Apr_4_2021_s55_c320", "gold_span": "MPQA Subjectivity Cues Lexicon MPQA Subjectivity Cues Lexicon Home page: https://mpqa.cs.pitt.edu/lexicons/subj_lexicon/ 6885 words from 8221 lemmas, annotated for intensity (strong/weak) 2718 positive 4912 negative + : admirable, beautiful, confident, dazzling, ecstatic, favor, glee, great − : awful, bad, bias, catastrophe, cheat, deny, envious, foul, harsh, hate 55 Theresa Wilson, Janyce Wiebe, and Paul Hoffmann (2005). Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis. Proc. of HLT-EMNLP-2005. Riloff and Wiebe (2003). Learning extraction patterns for subjective expressions. EMNLP-2003.", "category": "extractive", "doc_version": "v1"}
{"qid": "q224_paraphrase", "query": "What is MPQA Subjectivity Cues Lexicon MPQA Subjectivity Cues Lexicon Home page?", "gold_chunk_id": "4_NB_Apr_4_2021_s55_c320", "gold_span": "MPQA Subjectivity Cues Lexicon MPQA Subjectivity Cues Lexicon Home page: https://mpqa.cs.pitt.edu/lexicons/subj_lexicon/ 6885 words from 8221 lemmas, annotated for intensity (strong/weak) 2718 positive 4912 negative + : admirable, beautiful, confident, dazzling, ecstatic, favor, glee, great − : awful, bad, bias, catastrophe, cheat, deny, envious, foul, harsh, hate 55 Theresa Wilson, Janyce Wiebe, and Paul Hoffmann (2005). Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis. Proc. of HLT-EMNLP-2005. Riloff and Wiebe (2003). Learning extraction patterns for subjective expressions. EMNLP-2003.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q225_paraphrase", "query": "Summarize the MPQA Subjectivity Cues Lexicon MPQA Subjectivity Cues Lexicon Home page.", "gold_chunk_id": "4_NB_Apr_4_2021_s55_c320", "gold_span": "MPQA Subjectivity Cues Lexicon MPQA Subjectivity Cues Lexicon Home page: https://mpqa.cs.pitt.edu/lexicons/subj_lexicon/ 6885 words from 8221 lemmas, annotated for intensity (strong/weak) 2718 positive 4912 negative + : admirable, beautiful, confident, dazzling, ecstatic, favor, glee, great − : awful, bad, bias, catastrophe, cheat, deny, envious, foul, harsh, hate 55 Theresa Wilson, Janyce Wiebe, and Paul Hoffmann (2005). Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis. Proc. of HLT-EMNLP-2005. Riloff and Wiebe (2003). Learning extraction patterns for subjective expressions. EMNLP-2003.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q226_extract", "query": "What is The General Inquirer The General Inquirer Home page?", "gold_chunk_id": "4_NB_Apr_4_2021_s56_c321", "gold_span": "The General Inquirer The General Inquirer Home page: http://www.wjh.harvard.edu/~inquirer List of Categories: http://www.wjh.harvard.edu/~inquirer/homecat.htm Spreadsheet: http://www.wjh.harvard.edu/~inquirer/inquirerbasic.xls Categories: Positiv (1915 words) and Negativ (2291 words) Strong vs Weak, Active vs Passive, Overstated versus Understated Pleasure, Pain, Virtue, Vice, Motivation, Cognitive Orientation, etc Free for Research Use Philip J. Stone, Dexter C Dunphy, Marshall S. Smith, Daniel M. Ogilvie. 1966. The General Inquirer: A Computer Approach to Content Analysis. MIT Press", "category": "extractive", "doc_version": "v1"}
{"qid": "q227_paraphrase", "query": "Give me the The General Inquirer The General Inquirer Home page.", "gold_chunk_id": "4_NB_Apr_4_2021_s56_c321", "gold_span": "The General Inquirer The General Inquirer Home page: http://www.wjh.harvard.edu/~inquirer List of Categories: http://www.wjh.harvard.edu/~inquirer/homecat.htm Spreadsheet: http://www.wjh.harvard.edu/~inquirer/inquirerbasic.xls Categories: Positiv (1915 words) and Negativ (2291 words) Strong vs Weak, Active vs Passive, Overstated versus Understated Pleasure, Pain, Virtue, Vice, Motivation, Cognitive Orientation, etc Free for Research Use Philip J. Stone, Dexter C Dunphy, Marshall S. Smith, Daniel M. Ogilvie. 1966. The General Inquirer: A Computer Approach to Content Analysis. MIT Press", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q228_paraphrase", "query": "Summarize the The General Inquirer The General Inquirer Home page.", "gold_chunk_id": "4_NB_Apr_4_2021_s56_c321", "gold_span": "The General Inquirer The General Inquirer Home page: http://www.wjh.harvard.edu/~inquirer List of Categories: http://www.wjh.harvard.edu/~inquirer/homecat.htm Spreadsheet: http://www.wjh.harvard.edu/~inquirer/inquirerbasic.xls Categories: Positiv (1915 words) and Negativ (2291 words) Strong vs Weak, Active vs Passive, Overstated versus Understated Pleasure, Pain, Virtue, Vice, Motivation, Cognitive Orientation, etc Free for Research Use Philip J. Stone, Dexter C Dunphy, Marshall S. Smith, Daniel M. Ogilvie. 1966. The General Inquirer: A Computer Approach to Content Analysis. MIT Press", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q229_extract", "query": "What is Naive Bayes in Other tasks?", "gold_chunk_id": "4_NB_Apr_4_2021_s58_c323", "gold_span": "Naive Bayes in Other tasks: Spam Filtering SpamAssassin Features: Mentions millions of (dollar) ((dollar) NN,NNN,NNN.NN) From: starts with many numbers Subject is all capitals HTML has a low ratio of text to image area \"One hundred percent guaranteed\" Claims you can be removed from the list", "category": "extractive", "doc_version": "v1"}
{"qid": "q230_paraphrase", "query": "Summarize the Naive Bayes in Other tasks.", "gold_chunk_id": "4_NB_Apr_4_2021_s58_c323", "gold_span": "Naive Bayes in Other tasks: Spam Filtering SpamAssassin Features: Mentions millions of (dollar) ((dollar) NN,NNN,NNN.NN) From: starts with many numbers Subject is all capitals HTML has a low ratio of text to image area \"One hundred percent guaranteed\" Claims you can be removed from the list", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q231_paraphrase", "query": "Can you tell me about the Naive Bayes in Other tasks?", "gold_chunk_id": "4_NB_Apr_4_2021_s58_c323", "gold_span": "Naive Bayes in Other tasks: Spam Filtering SpamAssassin Features: Mentions millions of (dollar) ((dollar) NN,NNN,NNN.NN) From: starts with many numbers Subject is all capitals HTML has a low ratio of text to image area \"One hundred percent guaranteed\" Claims you can be removed from the list", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q232_extract", "query": "What is Summary?", "gold_chunk_id": "4_NB_Apr_4_2021_s60_c325", "gold_span": "Summary: Naive Bayes is Not So Naive Very Fast, low storage requirements Work well with very small amounts of training data Robust to Irrelevant Features Irrelevant Features cancel each other without affecting results Very good in domains with many equally important features Decision Trees suffer from fragmentation in such cases – especially if little data Optimal if the independence assumptions hold: If assumed independence is correct, then it is the Bayes Optimal Classifier for problem A good dependable baseline for text classification But we will see other classifiers that give better accuracy Slide from Chris Manning", "category": "extractive", "doc_version": "v1"}
{"qid": "q233_paraphrase", "query": "Can you tell me about the Summary?", "gold_chunk_id": "4_NB_Apr_4_2021_s60_c325", "gold_span": "Summary: Naive Bayes is Not So Naive Very Fast, low storage requirements Work well with very small amounts of training data Robust to Irrelevant Features Irrelevant Features cancel each other without affecting results Very good in domains with many equally important features Decision Trees suffer from fragmentation in such cases – especially if little data Optimal if the independence assumptions hold: If assumed independence is correct, then it is the Bayes Optimal Classifier for problem A good dependable baseline for text classification But we will see other classifiers that give better accuracy Slide from Chris Manning", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q234_paraphrase", "query": "Please provide details about Summary.", "gold_chunk_id": "4_NB_Apr_4_2021_s60_c325", "gold_span": "Summary: Naive Bayes is Not So Naive Very Fast, low storage requirements Work well with very small amounts of training data Robust to Irrelevant Features Irrelevant Features cancel each other without affecting results Very good in domains with many equally important features Decision Trees suffer from fragmentation in such cases – especially if little data Optimal if the independence assumptions hold: If assumed independence is correct, then it is the Bayes Optimal Classifier for problem A good dependable baseline for text classification But we will see other classifiers that give better accuracy Slide from Chris Manning", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q235_extract", "query": "What is Evaluation?", "gold_chunk_id": "4_NB_Apr_4_2021_s71_c336", "gold_span": "Evaluation: Accuracy Evaluation: Accuracy Why don't we use accuracy as our metric? Imagine we saw 1 million tweets 100 of them talked about Delicious Pie Co. 999,900 talked about something else We could build a dumb classifier that just labels every tweet \"not about pie\" It would get 99.99% accuracy!!! Wow!!!! But useless! Doesn't return the comments we are looking for! That's why we use precision and recall instead", "category": "extractive", "doc_version": "v1"}
{"qid": "q236_paraphrase", "query": "Summarize the Evaluation.", "gold_chunk_id": "4_NB_Apr_4_2021_s71_c336", "gold_span": "Evaluation: Accuracy Evaluation: Accuracy Why don't we use accuracy as our metric? Imagine we saw 1 million tweets 100 of them talked about Delicious Pie Co. 999,900 talked about something else We could build a dumb classifier that just labels every tweet \"not about pie\" It would get 99.99% accuracy!!! Wow!!!! But useless! Doesn't return the comments we are looking for! That's why we use precision and recall instead", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q237_paraphrase", "query": "Can you tell me about the Evaluation?", "gold_chunk_id": "4_NB_Apr_4_2021_s71_c336", "gold_span": "Evaluation: Accuracy Evaluation: Accuracy Why don't we use accuracy as our metric? Imagine we saw 1 million tweets 100 of them talked about Delicious Pie Co. 999,900 talked about something else We could build a dumb classifier that just labels every tweet \"not about pie\" It would get 99.99% accuracy!!! Wow!!!! But useless! Doesn't return the comments we are looking for! That's why we use precision and recall instead", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q238_extract", "query": "What is Evaluation?", "gold_chunk_id": "4_NB_Apr_4_2021_s72_c337", "gold_span": "Evaluation: Precision Evaluation: Precision % of items the system detected (i.e., items the system labeled as positive) that are in fact positive (according to the human gold labels)", "category": "extractive", "doc_version": "v1"}
{"qid": "q239_paraphrase", "query": "Give me the Evaluation.", "gold_chunk_id": "4_NB_Apr_4_2021_s72_c337", "gold_span": "Evaluation: Precision Evaluation: Precision % of items the system detected (i.e., items the system labeled as positive) that are in fact positive (according to the human gold labels)", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q240_paraphrase", "query": "Summarize the Evaluation.", "gold_chunk_id": "4_NB_Apr_4_2021_s72_c337", "gold_span": "Evaluation: Precision Evaluation: Precision % of items the system detected (i.e., items the system labeled as positive) that are in fact positive (according to the human gold labels)", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q241_extract", "query": "What is Evaluation?", "gold_chunk_id": "4_NB_Apr_4_2021_s73_c338", "gold_span": "Evaluation: Recall Evaluation: Recall % of items actually present in the input that were correctly identified by the system.", "category": "extractive", "doc_version": "v1"}
{"qid": "q242_paraphrase", "query": "Give me the Evaluation.", "gold_chunk_id": "4_NB_Apr_4_2021_s73_c338", "gold_span": "Evaluation: Recall Evaluation: Recall % of items actually present in the input that were correctly identified by the system.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q243_paraphrase", "query": "Can you tell me about the Evaluation?", "gold_chunk_id": "4_NB_Apr_4_2021_s73_c338", "gold_span": "Evaluation: Recall Evaluation: Recall % of items actually present in the input that were correctly identified by the system.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q244_extract", "query": "What is A combined measure?", "gold_chunk_id": "4_NB_Apr_4_2021_s75_c340", "gold_span": "A combined measure: F F measure: a single number that combines P and R: We almost always use balanced F1 (i.e.,  = 1)", "category": "extractive", "doc_version": "v1"}
{"qid": "q245_paraphrase", "query": "Summarize the A combined measure.", "gold_chunk_id": "4_NB_Apr_4_2021_s75_c340", "gold_span": "A combined measure: F F measure: a single number that combines P and R: We almost always use balanced F1 (i.e.,  = 1)", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q246_paraphrase", "query": "Give me the A combined measure.", "gold_chunk_id": "4_NB_Apr_4_2021_s75_c340", "gold_span": "A combined measure: F F measure: a single number that combines P and R: We almost always use balanced F1 (i.e.,  = 1)", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q247_extract", "query": "What is Cross-validation?", "gold_chunk_id": "4_NB_Apr_4_2021_s77_c342", "gold_span": "Cross-validation: multiple splits Pool results over splits, Compute pooled dev performance", "category": "extractive", "doc_version": "v1"}
{"qid": "q248_paraphrase", "query": "Summarize the Cross-validation.", "gold_chunk_id": "4_NB_Apr_4_2021_s77_c342", "gold_span": "Cross-validation: multiple splits Pool results over splits, Compute pooled dev performance", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q249_paraphrase", "query": "Give me the Cross-validation.", "gold_chunk_id": "4_NB_Apr_4_2021_s77_c342", "gold_span": "Cross-validation: multiple splits Pool results over splits, Compute pooled dev performance", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q250_extract", "query": "What is Bootstrap example Bootstrap example Suppose?", "gold_chunk_id": "4_NB_Apr_4_2021_s97_c362", "gold_span": "Bootstrap example Bootstrap example Suppose: We have 10,000 test sets x(i) and a threshold of .01 And in only 47 of the test sets do we find that δ(x(i)) ≥ 2δ(x) The resulting p-value is .0047 This is smaller than .01, indicating δ (x) is indeed sufficiently surprising And we reject the null hypothesis and conclude A is better than B.", "category": "extractive", "doc_version": "v1"}
{"qid": "q251_paraphrase", "query": "What is Bootstrap example Bootstrap example Suppose?", "gold_chunk_id": "4_NB_Apr_4_2021_s97_c362", "gold_span": "Bootstrap example Bootstrap example Suppose: We have 10,000 test sets x(i) and a threshold of .01 And in only 47 of the test sets do we find that δ(x(i)) ≥ 2δ(x) The resulting p-value is .0047 This is smaller than .01, indicating δ (x) is indeed sufficiently surprising And we reject the null hypothesis and conclude A is better than B.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q252_paraphrase", "query": "Give me the Bootstrap example Bootstrap example Suppose.", "gold_chunk_id": "4_NB_Apr_4_2021_s97_c362", "gold_span": "Bootstrap example Bootstrap example Suppose: We have 10,000 test sets x(i) and a threshold of .01 And in only 47 of the test sets do we find that δ(x(i)) ≥ 2δ(x) The resulting p-value is .0047 This is smaller than .01, indicating δ (x) is indeed sufficiently surprising And we reject the null hypothesis and conclude A is better than B.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q253_extract", "query": "What is Probabilistic Language Modeling Goal?", "gold_chunk_id": "3_LM_Jan_08_2021_s3_c373", "gold_span": "Probabilistic Language Modeling Goal: compute the probability of a sentence or sequence of words: P(W) = P(w1,w2,w3,w4,w5…wn) Related task: probability of an upcoming word: P(w5|w1,w2,w3,w4) A model that computes either of these: P(W) or P(wn|w1,w2…wn-1) is called a language model. Better: the grammar But language model or LM is standard", "category": "extractive", "doc_version": "v1"}
{"qid": "q254_paraphrase", "query": "Give me the Probabilistic Language Modeling Goal.", "gold_chunk_id": "3_LM_Jan_08_2021_s3_c373", "gold_span": "Probabilistic Language Modeling Goal: compute the probability of a sentence or sequence of words: P(W) = P(w1,w2,w3,w4,w5…wn) Related task: probability of an upcoming word: P(w5|w1,w2,w3,w4) A model that computes either of these: P(W) or P(wn|w1,w2…wn-1) is called a language model. Better: the grammar But language model or LM is standard", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q255_paraphrase", "query": "Can you tell me about the Probabilistic Language Modeling Goal?", "gold_chunk_id": "3_LM_Jan_08_2021_s3_c373", "gold_span": "Probabilistic Language Modeling Goal: compute the probability of a sentence or sequence of words: P(W) = P(w1,w2,w3,w4,w5…wn) Related task: probability of an upcoming word: P(w5|w1,w2,w3,w4) A model that computes either of these: P(W) or P(wn|w1,w2…wn-1) is called a language model. Better: the grammar But language model or LM is standard", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q256_extract", "query": "What is How to compute P(W) How to compute this joint probability?", "gold_chunk_id": "3_LM_Jan_08_2021_s4_c374", "gold_span": "How to compute P(W) How to compute this joint probability: P(its, water, is, so, transparent, that) Intuition: let’s rely on the Chain Rule of Probability", "category": "extractive", "doc_version": "v1"}
{"qid": "q257_paraphrase", "query": "Can you tell me about the How to compute P(W) How to compute this joint probability?", "gold_chunk_id": "3_LM_Jan_08_2021_s4_c374", "gold_span": "How to compute P(W) How to compute this joint probability: P(its, water, is, so, transparent, that) Intuition: let’s rely on the Chain Rule of Probability", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q258_paraphrase", "query": "What is How to compute P(W) How to compute this joint probability?", "gold_chunk_id": "3_LM_Jan_08_2021_s4_c374", "gold_span": "How to compute P(W) How to compute this joint probability: P(its, water, is, so, transparent, that) Intuition: let’s rely on the Chain Rule of Probability", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q259_extract", "query": "What is Reminder?", "gold_chunk_id": "3_LM_Jan_08_2021_s5_c375", "gold_span": "Reminder: The Chain Rule Recall the definition of conditional probabilities p(B|A) = P(A,B)/P(A) Rewriting: P(A,B) = P(A)P(B|A) More variables: P(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C) The Chain Rule in General P(x1,x2,x3,…,xn) = P(x1)P(x2|x1)P(x3|x1,x2)…P(xn|x1,…,xn-1)", "category": "extractive", "doc_version": "v1"}
{"qid": "q260_paraphrase", "query": "Please provide details about Reminder.", "gold_chunk_id": "3_LM_Jan_08_2021_s5_c375", "gold_span": "Reminder: The Chain Rule Recall the definition of conditional probabilities p(B|A) = P(A,B)/P(A) Rewriting: P(A,B) = P(A)P(B|A) More variables: P(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C) The Chain Rule in General P(x1,x2,x3,…,xn) = P(x1)P(x2|x1)P(x3|x1,x2)…P(xn|x1,…,xn-1)", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q261_paraphrase", "query": "Summarize the Reminder.", "gold_chunk_id": "3_LM_Jan_08_2021_s5_c375", "gold_span": "Reminder: The Chain Rule Recall the definition of conditional probabilities p(B|A) = P(A,B)/P(A) Rewriting: P(A,B) = P(A)P(B|A) More variables: P(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C) The Chain Rule in General P(x1,x2,x3,…,xn) = P(x1)P(x2|x1)P(x3|x1,x2)…P(xn|x1,…,xn-1)", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q262_extract", "query": "What is Markov Assumption Simplifying assumption?", "gold_chunk_id": "3_LM_Jan_08_2021_s8_c378", "gold_span": "Markov Assumption Simplifying assumption: Or maybe Andrei Markov", "category": "extractive", "doc_version": "v1"}
{"qid": "q263_paraphrase", "query": "Please provide details about Markov Assumption Simplifying assumption.", "gold_chunk_id": "3_LM_Jan_08_2021_s8_c378", "gold_span": "Markov Assumption Simplifying assumption: Or maybe Andrei Markov", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q264_paraphrase", "query": "Give me the Markov Assumption Simplifying assumption.", "gold_chunk_id": "3_LM_Jan_08_2021_s8_c378", "gold_span": "Markov Assumption Simplifying assumption: Or maybe Andrei Markov", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q265_extract", "query": "What is Simplest case?", "gold_chunk_id": "3_LM_Jan_08_2021_s10_c380", "gold_span": "Simplest case: Unigram model fifth, an, of, futures, the, an, incorporated, a, a, the, inflation, most, dollars, quarter, in, is, mass thrift, did, eighty, said, hard, 'm, july, bullish that, or, limited, the Some automatically generated sentences from a unigram model", "category": "extractive", "doc_version": "v1"}
{"qid": "q266_paraphrase", "query": "Summarize the Simplest case.", "gold_chunk_id": "3_LM_Jan_08_2021_s10_c380", "gold_span": "Simplest case: Unigram model fifth, an, of, futures, the, an, incorporated, a, a, the, inflation, most, dollars, quarter, in, is, mass thrift, did, eighty, said, hard, 'm, july, bullish that, or, limited, the Some automatically generated sentences from a unigram model", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q267_paraphrase", "query": "Please provide details about Simplest case.", "gold_chunk_id": "3_LM_Jan_08_2021_s10_c380", "gold_span": "Simplest case: Unigram model fifth, an, of, futures, the, an, incorporated, a, a, the, inflation, most, dollars, quarter, in, is, mass thrift, did, eighty, said, hard, 'm, july, bullish that, or, limited, the Some automatically generated sentences from a unigram model", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q268_extract", "query": "What is Condition on the previous word?", "gold_chunk_id": "3_LM_Jan_08_2021_s11_c381", "gold_span": "Condition on the previous word: Bigram model texaco, rose, one, in, this, issue, is, pursuing, growth, in, a, boiler, house, said, mr., gurria, mexico, 's, motion, control, proposal, without, permission, from, five, hundred, fifty, five, yen outside, new, car, parking, lot, of, the, agreement, reached this, would, be, a, record, november", "category": "extractive", "doc_version": "v1"}
{"qid": "q269_paraphrase", "query": "What is Condition on the previous word?", "gold_chunk_id": "3_LM_Jan_08_2021_s11_c381", "gold_span": "Condition on the previous word: Bigram model texaco, rose, one, in, this, issue, is, pursuing, growth, in, a, boiler, house, said, mr., gurria, mexico, 's, motion, control, proposal, without, permission, from, five, hundred, fifty, five, yen outside, new, car, parking, lot, of, the, agreement, reached this, would, be, a, record, november", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q270_paraphrase", "query": "Summarize the Condition on the previous word.", "gold_chunk_id": "3_LM_Jan_08_2021_s11_c381", "gold_span": "Condition on the previous word: Bigram model texaco, rose, one, in, this, issue, is, pursuing, growth, in, a, boiler, house, said, mr., gurria, mexico, 's, motion, control, proposal, without, permission, from, five, hundred, fifty, five, yen outside, new, car, parking, lot, of, the, agreement, reached this, would, be, a, record, november", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q271_extract", "query": "What is More examples?", "gold_chunk_id": "3_LM_Jan_08_2021_s17_c387", "gold_span": "More examples: Berkeley Restaurant Project sentences can you tell me about any good cantonese restaurants close by mid priced thai food is what i’m looking for tell me about chez panisse can you give me a listing of the kinds of food that are available i’m looking for a good place to eat breakfast when is caffe venezia open during the day", "category": "extractive", "doc_version": "v1"}
{"qid": "q272_paraphrase", "query": "Summarize the More examples.", "gold_chunk_id": "3_LM_Jan_08_2021_s17_c387", "gold_span": "More examples: Berkeley Restaurant Project sentences can you tell me about any good cantonese restaurants close by mid priced thai food is what i’m looking for tell me about chez panisse can you give me a listing of the kinds of food that are available i’m looking for a good place to eat breakfast when is caffe venezia open during the day", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q273_paraphrase", "query": "Can you tell me about the More examples?", "gold_chunk_id": "3_LM_Jan_08_2021_s17_c387", "gold_span": "More examples: Berkeley Restaurant Project sentences can you tell me about any good cantonese restaurants close by mid priced thai food is what i’m looking for tell me about chez panisse can you give me a listing of the kinds of food that are available i’m looking for a good place to eat breakfast when is caffe venezia open during the day", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q274_extract", "query": "What is Raw bigram probabilities Normalize by unigrams?", "gold_chunk_id": "3_LM_Jan_08_2021_s19_c389", "gold_span": "Raw bigram probabilities Normalize by unigrams: Result:", "category": "extractive", "doc_version": "v1"}
{"qid": "q275_paraphrase", "query": "Summarize the Raw bigram probabilities Normalize by unigrams.", "gold_chunk_id": "3_LM_Jan_08_2021_s19_c389", "gold_span": "Raw bigram probabilities Normalize by unigrams: Result:", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q276_paraphrase", "query": "Can you tell me about the Raw bigram probabilities Normalize by unigrams?", "gold_chunk_id": "3_LM_Jan_08_2021_s19_c389", "gold_span": "Raw bigram probabilities Normalize by unigrams: Result:", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q277_extract", "query": "What is Language Modeling Toolkits SRILM http?", "gold_chunk_id": "3_LM_Jan_08_2021_s23_c393", "gold_span": "Language Modeling Toolkits SRILM http: //www.speech.sri.com/projects/srilm/ KenLM https://kheafield.com/code/kenlm/", "category": "extractive", "doc_version": "v1"}
{"qid": "q278_paraphrase", "query": "Please provide details about Language Modeling Toolkits SRILM http.", "gold_chunk_id": "3_LM_Jan_08_2021_s23_c393", "gold_span": "Language Modeling Toolkits SRILM http: //www.speech.sri.com/projects/srilm/ KenLM https://kheafield.com/code/kenlm/", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q279_paraphrase", "query": "Give me the Language Modeling Toolkits SRILM http.", "gold_chunk_id": "3_LM_Jan_08_2021_s23_c393", "gold_span": "Language Modeling Toolkits SRILM http: //www.speech.sri.com/projects/srilm/ KenLM https://kheafield.com/code/kenlm/", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q280_extract", "query": "What is Google Book N-grams Google Book N-grams http?", "gold_chunk_id": "3_LM_Jan_08_2021_s26_c396", "gold_span": "Google Book N-grams Google Book N-grams http: //ngrams.googlelabs.com/", "category": "extractive", "doc_version": "v1"}
{"qid": "q281_paraphrase", "query": "Summarize the Google Book N-grams Google Book N-grams http.", "gold_chunk_id": "3_LM_Jan_08_2021_s26_c396", "gold_span": "Google Book N-grams Google Book N-grams http: //ngrams.googlelabs.com/", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q282_paraphrase", "query": "Can you tell me about the Google Book N-grams Google Book N-grams http?", "gold_chunk_id": "3_LM_Jan_08_2021_s26_c396", "gold_span": "Google Book N-grams Google Book N-grams http: //ngrams.googlelabs.com/", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q283_extract", "query": "What is Evaluation?", "gold_chunk_id": "3_LM_Jan_08_2021_s29_c399", "gold_span": "Evaluation: How good is our model? Does our language model prefer good sentences to bad ones? Assign higher probability to “real” or “frequently observed” sentences Than “ungrammatical” or “rarely observed” sentences? We train parameters of our model on a training set. We test the model’s performance on data we haven’t seen. A test set is an unseen dataset that is different from our training set, totally unused. An evaluation metric tells us how well our model does on the test set.", "category": "extractive", "doc_version": "v1"}
{"qid": "q284_paraphrase", "query": "Please provide details about Evaluation.", "gold_chunk_id": "3_LM_Jan_08_2021_s29_c399", "gold_span": "Evaluation: How good is our model? Does our language model prefer good sentences to bad ones? Assign higher probability to “real” or “frequently observed” sentences Than “ungrammatical” or “rarely observed” sentences? We train parameters of our model on a training set. We test the model’s performance on data we haven’t seen. A test set is an unseen dataset that is different from our training set, totally unused. An evaluation metric tells us how well our model does on the test set.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q285_paraphrase", "query": "Can you tell me about the Evaluation?", "gold_chunk_id": "3_LM_Jan_08_2021_s29_c399", "gold_span": "Evaluation: How good is our model? Does our language model prefer good sentences to bad ones? Assign higher probability to “real” or “frequently observed” sentences Than “ungrammatical” or “rarely observed” sentences? We train parameters of our model on a training set. We test the model’s performance on data we haven’t seen. A test set is an unseen dataset that is different from our training set, totally unused. An evaluation metric tells us how well our model does on the test set.", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q286_extract", "query": "What is Intuition of Perplexity Intuition of Perplexity The Shannon Game?", "gold_chunk_id": "3_LM_Jan_08_2021_s32_c402", "gold_span": "Intuition of Perplexity Intuition of Perplexity The Shannon Game: How well can we predict the next word? Unigrams are terrible at this game. (Why?) A better model of a text is one which assigns a higher probability to the word that actually occurs I always order pizza with cheese and ____ The 33rd President of the US was ____ I saw a ____ mushrooms 0.1 pepperoni 0.1 anchovies 0.01 …. fried rice 0.0001 …. and 1e-100", "category": "extractive", "doc_version": "v1"}
{"qid": "q287_paraphrase", "query": "Please provide details about Intuition of Perplexity Intuition of Perplexity The Shannon Game.", "gold_chunk_id": "3_LM_Jan_08_2021_s32_c402", "gold_span": "Intuition of Perplexity Intuition of Perplexity The Shannon Game: How well can we predict the next word? Unigrams are terrible at this game. (Why?) A better model of a text is one which assigns a higher probability to the word that actually occurs I always order pizza with cheese and ____ The 33rd President of the US was ____ I saw a ____ mushrooms 0.1 pepperoni 0.1 anchovies 0.01 …. fried rice 0.0001 …. and 1e-100", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q288_paraphrase", "query": "What is Intuition of Perplexity Intuition of Perplexity The Shannon Game?", "gold_chunk_id": "3_LM_Jan_08_2021_s32_c402", "gold_span": "Intuition of Perplexity Intuition of Perplexity The Shannon Game: How well can we predict the next word? Unigrams are terrible at this game. (Why?) A better model of a text is one which assigns a higher probability to the word that actually occurs I always order pizza with cheese and ____ The 33rd President of the US was ____ I saw a ____ mushrooms 0.1 pepperoni 0.1 anchovies 0.01 …. fried rice 0.0001 …. and 1e-100", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q289_extract", "query": "What is The Shannon Game intuition for perplexity Josh Goodman?", "gold_chunk_id": "3_LM_Jan_08_2021_s35_c405", "gold_span": "The Shannon Game intuition for perplexity Josh Goodman: imagine a call-routing phone system gets 120K calls and has to recognize \"Operator\" (let's say this occurs 1 in 4 calls) \"Sales\" (1in 4) \"Technical Support\" (1 in 4) 30,000 different names (each name occurring 1 time in the 120K calls) We get the perplexity of this sequence of length 120Kby first multiplying 120K probabilities (90K of which are 1/4 and 30K of which are 1/120K), nd then taking the inverse 120,000th root: Perp = (¼ * ¼ * ¼* ¼ * ¼ * …. * 1/120K * 1/120K * ….)^(-1/120K) But this can be arithmetically simplified to just N = 4: the operator (1/4), the sales (1/4), the tech support (1/4), and the 30,000 names (1/120,000): Perplexity= ((¼ * ¼ * ¼ * 1/120K)^(-1/4) = 52.6", "category": "extractive", "doc_version": "v1"}
{"qid": "q290_paraphrase", "query": "What is The Shannon Game intuition for perplexity Josh Goodman?", "gold_chunk_id": "3_LM_Jan_08_2021_s35_c405", "gold_span": "The Shannon Game intuition for perplexity Josh Goodman: imagine a call-routing phone system gets 120K calls and has to recognize \"Operator\" (let's say this occurs 1 in 4 calls) \"Sales\" (1in 4) \"Technical Support\" (1 in 4) 30,000 different names (each name occurring 1 time in the 120K calls) We get the perplexity of this sequence of length 120Kby first multiplying 120K probabilities (90K of which are 1/4 and 30K of which are 1/120K), nd then taking the inverse 120,000th root: Perp = (¼ * ¼ * ¼* ¼ * ¼ * …. * 1/120K * 1/120K * ….)^(-1/120K) But this can be arithmetically simplified to just N = 4: the operator (1/4), the sales (1/4), the tech support (1/4), and the 30,000 names (1/120,000): Perplexity= ((¼ * ¼ * ¼ * 1/120K)^(-1/4) = 52.6", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q291_paraphrase", "query": "Give me the The Shannon Game intuition for perplexity Josh Goodman.", "gold_chunk_id": "3_LM_Jan_08_2021_s35_c405", "gold_span": "The Shannon Game intuition for perplexity Josh Goodman: imagine a call-routing phone system gets 120K calls and has to recognize \"Operator\" (let's say this occurs 1 in 4 calls) \"Sales\" (1in 4) \"Technical Support\" (1 in 4) 30,000 different names (each name occurring 1 time in the 120K calls) We get the perplexity of this sequence of length 120Kby first multiplying 120K probabilities (90K of which are 1/4 and 30K of which are 1/120K), nd then taking the inverse 120,000th root: Perp = (¼ * ¼ * ¼* ¼ * ¼ * …. * 1/120K * 1/120K * ….)^(-1/120K) But this can be arithmetically simplified to just N = 4: the operator (1/4), the sales (1/4), the tech support (1/4), and the 30,000 names (1/120,000): Perplexity= ((¼ * ¼ * ¼ * 1/120K)^(-1/4) = 52.6", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q292_extract", "query": "What is Zeros Training set?", "gold_chunk_id": "3_LM_Jan_08_2021_s46_c416", "gold_span": "Zeros Training set: … denied the allegations … denied the reports … denied the claims … denied the request P(“offer” | denied the) = 0 Test set … denied the offer … denied the loan", "category": "extractive", "doc_version": "v1"}
{"qid": "q293_paraphrase", "query": "Summarize the Zeros Training set.", "gold_chunk_id": "3_LM_Jan_08_2021_s46_c416", "gold_span": "Zeros Training set: … denied the allegations … denied the reports … denied the claims … denied the request P(“offer” | denied the) = 0 Test set … denied the offer … denied the loan", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q294_paraphrase", "query": "Give me the Zeros Training set.", "gold_chunk_id": "3_LM_Jan_08_2021_s46_c416", "gold_span": "Zeros Training set: … denied the allegations … denied the reports … denied the claims … denied the request P(“offer” | denied the) = 0 Test set … denied the offer … denied the loan", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q295_extract", "query": "What is Language Modeling Smoothing?", "gold_chunk_id": "3_LM_Jan_08_2021_s49_c419", "gold_span": "Language Modeling Smoothing: Add-one (Laplace) smoothing", "category": "extractive", "doc_version": "v1"}
{"qid": "q296_paraphrase", "query": "What is Language Modeling Smoothing?", "gold_chunk_id": "3_LM_Jan_08_2021_s49_c419", "gold_span": "Language Modeling Smoothing: Add-one (Laplace) smoothing", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q297_paraphrase", "query": "Summarize the Language Modeling Smoothing.", "gold_chunk_id": "3_LM_Jan_08_2021_s49_c419", "gold_span": "Language Modeling Smoothing: Add-one (Laplace) smoothing", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q298_extract", "query": "What is The intuition of smoothing (from Dan Klein) When we have sparse statistics?", "gold_chunk_id": "3_LM_Jan_08_2021_s50_c420", "gold_span": "The intuition of smoothing (from Dan Klein) When we have sparse statistics: Steal probability mass to generalize better P(w | denied the) 3 allegations 2 reports 1 claims 1 request 7 total P(w | denied the) 2.5 allegations 1.5 reports 0.5 claims 0.5 request 2 other 7 total allegations reports claims attack request man outcome … allegations attack man outcome … allegations reports claims request", "category": "extractive", "doc_version": "v1"}
{"qid": "q299_paraphrase", "query": "Give me the The intuition of smoothing (from Dan Klein) When we have sparse statistics.", "gold_chunk_id": "3_LM_Jan_08_2021_s50_c420", "gold_span": "The intuition of smoothing (from Dan Klein) When we have sparse statistics: Steal probability mass to generalize better P(w | denied the) 3 allegations 2 reports 1 claims 1 request 7 total P(w | denied the) 2.5 allegations 1.5 reports 0.5 claims 0.5 request 2 other 7 total allegations reports claims attack request man outcome … allegations attack man outcome … allegations reports claims request", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q300_paraphrase", "query": "Please provide details about The intuition of smoothing (from Dan Klein) When we have sparse statistics.", "gold_chunk_id": "3_LM_Jan_08_2021_s50_c420", "gold_span": "The intuition of smoothing (from Dan Klein) When we have sparse statistics: Steal probability mass to generalize better P(w | denied the) 3 allegations 2 reports 1 claims 1 request 7 total P(w | denied the) 2.5 allegations 1.5 reports 0.5 claims 0.5 request 2 other 7 total allegations reports claims attack request man outcome … allegations attack man outcome … allegations reports claims request", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q301_extract", "query": "What is Berkeley Restaurant Corpus?", "gold_chunk_id": "3_LM_Jan_08_2021_s53_c423", "gold_span": "Berkeley Restaurant Corpus: Laplace smoothed bigram counts", "category": "extractive", "doc_version": "v1"}
{"qid": "q302_paraphrase", "query": "Summarize the Berkeley Restaurant Corpus.", "gold_chunk_id": "3_LM_Jan_08_2021_s53_c423", "gold_span": "Berkeley Restaurant Corpus: Laplace smoothed bigram counts", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q303_paraphrase", "query": "Please provide details about Berkeley Restaurant Corpus.", "gold_chunk_id": "3_LM_Jan_08_2021_s53_c423", "gold_span": "Berkeley Restaurant Corpus: Laplace smoothed bigram counts", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q304_extract", "query": "What is Language Modeling Smoothing?", "gold_chunk_id": "3_LM_Jan_08_2021_s58_c428", "gold_span": "Language Modeling Smoothing: Add-one (Laplace) smoothing", "category": "extractive", "doc_version": "v1"}
{"qid": "q305_paraphrase", "query": "Please provide details about Language Modeling Smoothing.", "gold_chunk_id": "3_LM_Jan_08_2021_s58_c428", "gold_span": "Language Modeling Smoothing: Add-one (Laplace) smoothing", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q306_paraphrase", "query": "What is Language Modeling Smoothing?", "gold_chunk_id": "3_LM_Jan_08_2021_s58_c428", "gold_span": "Language Modeling Smoothing: Add-one (Laplace) smoothing", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q307_extract", "query": "What is Unknown words?", "gold_chunk_id": "3_LM_Jan_08_2021_s63_c433", "gold_span": "Unknown words: Open versus closed vocabulary tasks If we know all the words in advanced Vocabulary V is fixed Closed vocabulary task Often we don’t know this Out Of Vocabulary = OOV words Open vocabulary task Instead: create an unknown word token <UNK> Training of <UNK> probabilities Create a fixed lexicon L of size V At text normalization phase, any training word not in L changed to <UNK> Now we train its probabilities like a normal word At decoding time If text input: Use UNK probabilities for any word not in training", "category": "extractive", "doc_version": "v1"}
{"qid": "q308_paraphrase", "query": "What is Unknown words?", "gold_chunk_id": "3_LM_Jan_08_2021_s63_c433", "gold_span": "Unknown words: Open versus closed vocabulary tasks If we know all the words in advanced Vocabulary V is fixed Closed vocabulary task Often we don’t know this Out Of Vocabulary = OOV words Open vocabulary task Instead: create an unknown word token <UNK> Training of <UNK> probabilities Create a fixed lexicon L of size V At text normalization phase, any training word not in L changed to <UNK> Now we train its probabilities like a normal word At decoding time If text input: Use UNK probabilities for any word not in training", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q309_paraphrase", "query": "Give me the Unknown words.", "gold_chunk_id": "3_LM_Jan_08_2021_s63_c433", "gold_span": "Unknown words: Open versus closed vocabulary tasks If we know all the words in advanced Vocabulary V is fixed Closed vocabulary task Often we don’t know this Out Of Vocabulary = OOV words Open vocabulary task Instead: create an unknown word token <UNK> Training of <UNK> probabilities Create a fixed lexicon L of size V At text normalization phase, any training word not in L changed to <UNK> Now we train its probabilities like a normal word At decoding time If text input: Use UNK probabilities for any word not in training", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q310_extract", "query": "What is N-gram Smoothing Summary N-gram Smoothing Summary Add-1 smoothing?", "gold_chunk_id": "3_LM_Jan_08_2021_s66_c436", "gold_span": "N-gram Smoothing Summary N-gram Smoothing Summary Add-1 smoothing: OK for text categorization, not for language modeling The most commonly used method: Extended Interpolated Kneser-Ney For very large N-grams like the Web: Stupid backoff 66", "category": "extractive", "doc_version": "v1"}
{"qid": "q311_paraphrase", "query": "Please provide details about N-gram Smoothing Summary N-gram Smoothing Summary Add-1 smoothing.", "gold_chunk_id": "3_LM_Jan_08_2021_s66_c436", "gold_span": "N-gram Smoothing Summary N-gram Smoothing Summary Add-1 smoothing: OK for text categorization, not for language modeling The most commonly used method: Extended Interpolated Kneser-Ney For very large N-grams like the Web: Stupid backoff 66", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q312_paraphrase", "query": "Can you tell me about the N-gram Smoothing Summary N-gram Smoothing Summary Add-1 smoothing?", "gold_chunk_id": "3_LM_Jan_08_2021_s66_c436", "gold_span": "N-gram Smoothing Summary N-gram Smoothing Summary Add-1 smoothing: OK for text categorization, not for language modeling The most commonly used method: Extended Interpolated Kneser-Ney For very large N-grams like the Web: Stupid backoff 66", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q313_extract", "query": "What is Advanced Language Modeling Discriminative models?", "gold_chunk_id": "3_LM_Jan_08_2021_s67_c437", "gold_span": "Advanced Language Modeling Discriminative models: choose n-gram weights to improve a task, not to fit the training set Parsing-based models Caching Models Recently used words are more likely to appear These turned out to perform very poorly for speech recognition (why?)", "category": "extractive", "doc_version": "v1"}
{"qid": "q314_paraphrase", "query": "Can you tell me about the Advanced Language Modeling Discriminative models?", "gold_chunk_id": "3_LM_Jan_08_2021_s67_c437", "gold_span": "Advanced Language Modeling Discriminative models: choose n-gram weights to improve a task, not to fit the training set Parsing-based models Caching Models Recently used words are more likely to appear These turned out to perform very poorly for speech recognition (why?)", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q315_paraphrase", "query": "Summarize the Advanced Language Modeling Discriminative models.", "gold_chunk_id": "3_LM_Jan_08_2021_s67_c437", "gold_span": "Advanced Language Modeling Discriminative models: choose n-gram weights to improve a task, not to fit the training set Parsing-based models Caching Models Recently used words are more likely to appear These turned out to perform very poorly for speech recognition (why?)", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q316_extract", "query": "What is Language Modeling Advanced?", "gold_chunk_id": "3_LM_Jan_08_2021_s69_c439", "gold_span": "Language Modeling Advanced: Kneser-Ney Smoothing", "category": "extractive", "doc_version": "v1"}
{"qid": "q317_paraphrase", "query": "What is Language Modeling Advanced?", "gold_chunk_id": "3_LM_Jan_08_2021_s69_c439", "gold_span": "Language Modeling Advanced: Kneser-Ney Smoothing", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q318_paraphrase", "query": "Please provide details about Language Modeling Advanced.", "gold_chunk_id": "3_LM_Jan_08_2021_s69_c439", "gold_span": "Language Modeling Advanced: Kneser-Ney Smoothing", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q319_extract", "query": "What is Absolute discounting?", "gold_chunk_id": "3_LM_Jan_08_2021_s70_c440", "gold_span": "Absolute discounting: just subtract a little from each count Suppose we wanted to subtract a little from a count of 4 to save probability mass for the zeros How much to subtract ? Church and Gale (1991)’s clever idea Divide up 22 million words of AP Newswire Training and held-out set for each bigram in the training set see the actual count in the held-out set! It sure looks like c* = (c - .75) Bigram count in training | Bigram count in heldout set 0 | .0000270 1 | 0.448 2 | 1.25 3 | 2.24 4 | 3.23 5 | 4.21 6 | 5.23 7 | 6.21 8 | 7.21 9 | 8.26", "category": "extractive", "doc_version": "v1"}
{"qid": "q320_paraphrase", "query": "Please provide details about Absolute discounting.", "gold_chunk_id": "3_LM_Jan_08_2021_s70_c440", "gold_span": "Absolute discounting: just subtract a little from each count Suppose we wanted to subtract a little from a count of 4 to save probability mass for the zeros How much to subtract ? Church and Gale (1991)’s clever idea Divide up 22 million words of AP Newswire Training and held-out set for each bigram in the training set see the actual count in the held-out set! It sure looks like c* = (c - .75) Bigram count in training | Bigram count in heldout set 0 | .0000270 1 | 0.448 2 | 1.25 3 | 2.24 4 | 3.23 5 | 4.21 6 | 5.23 7 | 6.21 8 | 7.21 9 | 8.26", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q321_paraphrase", "query": "Can you tell me about the Absolute discounting?", "gold_chunk_id": "3_LM_Jan_08_2021_s70_c440", "gold_span": "Absolute discounting: just subtract a little from each count Suppose we wanted to subtract a little from a count of 4 to save probability mass for the zeros How much to subtract ? Church and Gale (1991)’s clever idea Divide up 22 million words of AP Newswire Training and held-out set for each bigram in the training set see the actual count in the held-out set! It sure looks like c* = (c - .75) Bigram count in training | Bigram count in heldout set 0 | .0000270 1 | 0.448 2 | 1.25 3 | 2.24 4 | 3.23 5 | 4.21 6 | 5.23 7 | 6.21 8 | 7.21 9 | 8.26", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q322_extract", "query": "What is Kneser-Ney Smoothing II How many times does w appear as a novel continuation?", "gold_chunk_id": "3_LM_Jan_08_2021_s73_c443", "gold_span": "Kneser-Ney Smoothing II How many times does w appear as a novel continuation: Normalized by the total number of word bigram types", "category": "extractive", "doc_version": "v1"}
{"qid": "q323_paraphrase", "query": "Summarize the Kneser-Ney Smoothing II How many times does w appear as a novel continuation.", "gold_chunk_id": "3_LM_Jan_08_2021_s73_c443", "gold_span": "Kneser-Ney Smoothing II How many times does w appear as a novel continuation: Normalized by the total number of word bigram types", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q324_paraphrase", "query": "Give me the Kneser-Ney Smoothing II How many times does w appear as a novel continuation.", "gold_chunk_id": "3_LM_Jan_08_2021_s73_c443", "gold_span": "Kneser-Ney Smoothing II How many times does w appear as a novel continuation: Normalized by the total number of word bigram types", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q325_extract", "query": "What is Kneser-Ney Smoothing III Alternative metaphor?", "gold_chunk_id": "3_LM_Jan_08_2021_s74_c444", "gold_span": "Kneser-Ney Smoothing III Alternative metaphor: The number of # of word types seen to precede w normalized by the # of words preceding all words: A frequent word (Kong) occurring in only one context (Hong) will have a low continuation probability", "category": "extractive", "doc_version": "v1"}
{"qid": "q326_paraphrase", "query": "Summarize the Kneser-Ney Smoothing III Alternative metaphor.", "gold_chunk_id": "3_LM_Jan_08_2021_s74_c444", "gold_span": "Kneser-Ney Smoothing III Alternative metaphor: The number of # of word types seen to precede w normalized by the # of words preceding all words: A frequent word (Kong) occurring in only one context (Hong) will have a low continuation probability", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q327_paraphrase", "query": "Give me the Kneser-Ney Smoothing III Alternative metaphor.", "gold_chunk_id": "3_LM_Jan_08_2021_s74_c444", "gold_span": "Kneser-Ney Smoothing III Alternative metaphor: The number of # of word types seen to precede w normalized by the # of words preceding all words: A frequent word (Kong) occurring in only one context (Hong) will have a low continuation probability", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q328_extract", "query": "What is Kneser-Ney Smoothing?", "gold_chunk_id": "3_LM_Jan_08_2021_s76_c446", "gold_span": "Kneser-Ney Smoothing: Recursive formulation Kneser-Ney Smoothing: Recursive formulation 76 Continuation count = Number of unique single word contexts for ", "category": "extractive", "doc_version": "v1"}
{"qid": "q329_paraphrase", "query": "What is Kneser-Ney Smoothing?", "gold_chunk_id": "3_LM_Jan_08_2021_s76_c446", "gold_span": "Kneser-Ney Smoothing: Recursive formulation Kneser-Ney Smoothing: Recursive formulation 76 Continuation count = Number of unique single word contexts for ", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q330_paraphrase", "query": "Summarize the Kneser-Ney Smoothing.", "gold_chunk_id": "3_LM_Jan_08_2021_s76_c446", "gold_span": "Kneser-Ney Smoothing: Recursive formulation Kneser-Ney Smoothing: Recursive formulation 76 Continuation count = Number of unique single word contexts for ", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q331_extract", "query": "What is Language Modeling Advanced?", "gold_chunk_id": "3_LM_Jan_08_2021_s77_c447", "gold_span": "Language Modeling Advanced: Kneser-Ney Smoothing", "category": "extractive", "doc_version": "v1"}
{"qid": "q332_paraphrase", "query": "What is Language Modeling Advanced?", "gold_chunk_id": "3_LM_Jan_08_2021_s77_c447", "gold_span": "Language Modeling Advanced: Kneser-Ney Smoothing", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q333_paraphrase", "query": "Please provide details about Language Modeling Advanced.", "gold_chunk_id": "3_LM_Jan_08_2021_s77_c447", "gold_span": "Language Modeling Advanced: Kneser-Ney Smoothing", "category": "paraphrase", "doc_version": "v1"}
{"qid": "q334_unanswerable", "query": "What is the parking permit policy?", "gold_chunk_id": null, "gold_span": "", "category": "unanswerable", "doc_version": "v1"}
{"qid": "q335_unanswerable", "query": "How do I request extra credit after the final?", "gold_chunk_id": null, "gold_span": "", "category": "unanswerable", "doc_version": "v1"}
{"qid": "q336_unanswerable", "query": "What’s the policy for Assignment 99?", "gold_chunk_id": null, "gold_span": "", "category": "unanswerable", "doc_version": "v1"}
{"qid": "q337_unanswerable", "query": "Where is the lab access key pickup?", "gold_chunk_id": null, "gold_span": "", "category": "unanswerable", "doc_version": "v1"}
{"qid": "q338_unanswerable", "query": "When is the optional retake exam?", "gold_chunk_id": null, "gold_span": "", "category": "unanswerable", "doc_version": "v1"}
{"qid": "q339_unanswerable", "query": "How do I waive the course fee?", "gold_chunk_id": null, "gold_span": "", "category": "unanswerable", "doc_version": "v1"}
{"qid": "q340_unanswerable", "query": "What is the bonus project deadline?", "gold_chunk_id": null, "gold_span": "", "category": "unanswerable", "doc_version": "v1"}
